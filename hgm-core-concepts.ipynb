{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HGM Core Concepts Workshop\n\nA hands-on exploration of AI agent memory and context management using Hierarchical Graph Memory (HGM) concepts.\n\n---\n\n## What You Will Learn\n\nThis workshop teaches the foundational concepts that enable AI agents to **remember**, **learn**, and **adapt**. By the end, you'll understand:\n\n| Concept | What It Is | Why It Matters |\n|---------|------------|----------------|\n| **Context Engineering** | Techniques for managing what information an agent \"sees\" | Agents have limited context windows; smart selection is critical |\n| **Agent State Management** | Tracking focus, sessions, and conversation episodes | Enables personalized, coherent interactions over time |\n| **Agentic RAG** | Retrieval-Augmented Generation with active memory promotion | Goes beyond static retrieval to dynamically reorganize knowledge |\n| **Hierarchical Memory** | Three-tier architecture with temperature-based placement | Balances speed vs. storage, like human working/long-term memory |\n\n---\n\n## How This Workshop Is Structured\n\nEach section follows a consistent pattern:\n1. **Concept Definition** - What is this concept and why does it exist?\n2. **Comparison to Literature** - How does this relate to established patterns?\n3. **Algorithm Explanation** - The underlying algorithms with pseudocode\n4. **Code Implementation** - Working code you can modify and experiment with\n5. **Demo Output** - See the concepts in action\n\n---\n\n## Prerequisites\n\n- **Python 3.10+**\n- **NumPy** (the only external dependency)\n- Basic understanding of AI/LLM concepts\n\n**No external databases, Redis, or API keys needed!** Everything runs in-memory for workshop portability.\n\n---\n\n## Key Insight\n\n> Traditional RAG systems are **passive** - they retrieve the same documents regardless of context.\n> \n> HGM is **active** - it reorganizes memories based on what's relevant NOW, promoting frequently-accessed knowledge and letting unused knowledge fade.\n\nThis mirrors how human memory works: information you use often stays accessible, while rarely-used knowledge becomes harder to recall.\n\n---\n\n## Context Engineering Landscape\n\nBefore diving into HGM specifics, the next section situates these concepts within the broader **context engineering** landscape, comparing to:\n\n- **Memory Streams** (Generative Agents paper)\n- **MemGPT** (Virtual context management)\n- **Reflexion** (Self-reflection patterns)\n- **RAPTOR** (Recursive summarization)\n- Traditional **RAG** approaches\n\nThis will help you understand **what's new** in HGM and **why** each design decision was made."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Setup - Only standard library + numpy\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from enum import Enum\n",
    "from typing import Any, Optional\n",
    "import uuid\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Utility for timestamps\n",
    "def utcnow() -> datetime:\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Dependencies: numpy, dataclasses, enum, datetime, uuid, re\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Context Engineering: Where HGM Fits\n\n### What is Context Engineering?\n\n**Context Engineering** is the discipline of designing and managing the information that flows into an AI model's context window. As models have limited context (4K-200K tokens), what you include\u2014and exclude\u2014directly impacts response quality.\n\n> \"Context engineering is the new prompt engineering\" \u2014 Andrej Karpathy\n\n### The Evolution of Context Management\n\n| Era | Approach | Limitation |\n|-----|----------|------------|\n| **Prompt Engineering** | Hand-crafted prompts with examples | Static, doesn't scale |\n| **RAG v1** | Retrieve docs \u2192 stuff into context | No prioritization, token waste |\n| **RAG v2** | Chunking + reranking | Still passive, no learning |\n| **Agentic RAG** | Active retrieval with memory | Where HGM operates |\n| **Context Engineering** | Holistic context curation | Full system design |\n\n### Established Patterns vs HGM Implementation\n\n| Established Pattern | Description | HGM Implementation |\n|---------------------|-------------|-------------------|\n| **Sliding Window** | Keep last N tokens | Episode boundaries + focus decay |\n| **Retrieval-Augmented Generation** | Fetch relevant docs at query time | Three-tier memory with promotion |\n| **Memory Streams** (Park et al.) | Time-weighted memory with importance | Temperature scoring (recency \u00d7 relevance \u00d7 frequency) |\n| **Reflection** | Summarize and compress memories | Episode summaries + hierarchy paths |\n| **Working Memory** | Limited capacity, fast access store | Hot tier with token-budget LRU |\n| **Long-term Memory** | Persistent knowledge storage | Cold tier (PostgreSQL) |\n| **Semantic Memory** | Facts and concepts | SEMANTIC memory type |\n| **Episodic Memory** | Events and experiences | EPISODIC memory type + Episode tracking |\n| **Procedural Memory** | Skills and how-to knowledge | PROCEDURAL memory type |\n\n### Key Concepts from the Literature\n\n#### 1. Memory Streams (Stanford/Google - Generative Agents)\nThe influential \"Generative Agents\" paper introduced **memory streams** with:\n- **Recency**: Recent memories are more important\n- **Importance**: Some memories matter more\n- **Relevance**: Context-dependent retrieval\n\n**HGM's approach**: Temperature scoring combines all three into a single metric with tunable weights.\n\n\ud83d\udcc4 Paper: [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442) (Park et al., 2023)\n\n#### 2. Reflexion (Shinn et al.)\nSelf-reflection pattern where agents learn from mistakes:\n- Store failed attempts\n- Retrieve relevant failures\n- Avoid repeating mistakes\n\n**HGM's approach**: Pattern graph stores learned strategies with effectiveness scores that improve over time.\n\n\ud83d\udcc4 Paper: [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366) (Shinn et al., 2023)\n\n#### 3. MemGPT (Packer et al.)\nVirtual context management mimicking OS memory:\n- Main context = RAM (limited)\n- External storage = Disk (unlimited)\n- Explicit page in/out\n\n**HGM's approach**: Three-tier architecture (Hot/Warm/Cold) with automatic promotion/demotion based on temperature.\n\n\ud83d\udcc4 Paper: [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) (Packer et al., 2023)\n\n#### 4. RAPTOR (Sarthi et al.)\nRecursive summarization for better retrieval:\n- Build tree of summaries\n- Retrieve at appropriate abstraction level\n\n**HGM's approach**: Hierarchy paths (e.g., \"tech/python/async\") enable retrieval at different abstraction levels.\n\n\ud83d\udcc4 Paper: [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059) (Sarthi et al., 2024)\n\n#### 5. ACE - Agentic Context Engineering (Stanford)\nFramework for dynamically evolving contexts that enable self-improvement:\n- Contexts as \"evolving playbooks\" that accumulate strategies\n- Generation, reflection, and curation cycle\n- Addresses \"brevity bias\" and \"context collapse\"\n\n**HGM's approach**: Similar philosophy of treating context as dynamic rather than static, with pattern graphs that learn and improve over time.\n\n\ud83d\udcc4 Paper: [Agentic Context Engineering](https://arxiv.org/abs/2510.04618) (Agarwal et al., 2025)\n\n#### 6. Cognitive Architectures (ACT-R, SOAR)\nClassical AI architectures modeling human cognition:\n- Declarative vs procedural memory\n- Activation-based retrieval\n- Goal-directed behavior\n\n**HGM's approach**: Memory types (SEMANTIC, EPISODIC, PROCEDURAL, EMOTIONAL) directly inspired by cognitive science.\n\n\ud83d\udcc4 Resources: [ACT-R](http://act-r.psy.cmu.edu/) | [SOAR](https://soar.eecs.umich.edu/)\n\n### Context Window Strategies Comparison\n\n\n\n### HGM's Unique Contributions\n\n| Innovation | What's New | Why It Matters |\n|------------|------------|----------------|\n| **Temperature Scoring** | Unified 5-factor metric | Single score for all tier/ranking decisions |\n| **Automatic Promotion** | Access patterns drive organization | System improves without explicit training |\n| **Pattern Graph** | Keyword \u2192 Strategy mapping | Bypass LLM for known patterns |\n| **Mode Selection** | Route to optimal response strategy | Cost/latency optimization |\n| **Agent Context** | Per-agent focus tracking | Multi-agent personalization |\n\n### Research References & Further Reading\n\n| Paper | Year | Key Contribution | Link |\n|-------|------|------------------|------|\n| **Generative Agents** | 2023 | Memory streams, reflection, importance scoring | [arXiv:2304.03442](https://arxiv.org/abs/2304.03442) |\n| **MemGPT** | 2023 | Virtual context management, paging | [arXiv:2310.08560](https://arxiv.org/abs/2310.08560) |\n| **Reflexion** | 2023 | Self-reflection, learning from failures | [arXiv:2303.11366](https://arxiv.org/abs/2303.11366) |\n| **RAPTOR** | 2024 | Recursive summarization trees | [arXiv:2401.18059](https://arxiv.org/abs/2401.18059) |\n| **ACE** | 2025 | Evolving context playbooks, self-improving agents | [arXiv:2510.04618](https://arxiv.org/abs/2510.04618) |\n| **RAG Survey** | 2024 | Comprehensive RAG techniques overview | [arXiv:2312.10997](https://arxiv.org/abs/2312.10997) |\n| **Self-RAG** | 2023 | Self-reflective retrieval augmentation | [arXiv:2310.11511](https://arxiv.org/abs/2310.11511) |\n| **LangChain** | 2022+ | Practical RAG implementation patterns | [docs.langchain.com](https://docs.langchain.com) |\n| **LlamaIndex** | 2022+ | Data framework for LLM applications | [docs.llamaindex.ai](https://docs.llamaindex.ai) |\n| **ACT-R** | 1993+ | Cognitive architecture, activation-based memory | [act-r.psy.cmu.edu](http://act-r.psy.cmu.edu/) |\n| **SOAR** | 1987+ | Cognitive architecture, procedural learning | [soar.eecs.umich.edu](https://soar.eecs.umich.edu/) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 2: Core Data Models - Memory Types\n\n### The Concept\n\nHuman memory isn't a single system\u2014cognitive scientists have identified distinct memory types that serve different purposes. HGM adopts this model for AI agents, classifying memories to enable smarter retrieval and organization.\n\n### Why This Matters for AI Agents\n\nWithout memory classification:\n- \u274c A fact and a preference get treated identically\n- \u274c Instructions get buried alongside random observations\n- \u274c The agent can't prioritize procedural knowledge when executing tasks\n\nWith memory classification:\n- \u2705 Agent knows to retrieve PROCEDURAL memories when asked \"how to\"\n- \u2705 EMOTIONAL memories (preferences) influence response style\n- \u2705 EPISODIC memories provide conversation continuity\n\n### The Four Memory Types\n\n| Type | Cognitive Basis | AI Agent Use Case | Example |\n|------|-----------------|-------------------|---------|\n| **SEMANTIC** | Declarative facts, world knowledge | Reference information, definitions | \"Python is a programming language\" |\n| **EPISODIC** | Events tied to time/place | Conversation history, meeting notes | \"We discussed the API design yesterday\" |\n| **PROCEDURAL** | How-to knowledge, skills | Instructions, workflows, recipes | \"To deploy: run docker compose up\" |\n| **EMOTIONAL** | Feelings, preferences | User preferences, sentiment | \"User prefers concise explanations\" |\n\n### What the Code Demonstrates\n\n1. **MemoryType enum** - Classification system for memories\n2. **Memory dataclass** - The core unit with embedding, metadata, and temperature\n3. **create_memory()** - Factory function with deterministic mock embeddings\n\n### Mental Model\n\nThink of memory types like filing cabinets:\n- \ud83d\udcda **SEMANTIC** = Encyclopedia cabinet (facts to look up)\n- \ud83d\udcc5 **EPISODIC** = Journal cabinet (events to recall)\n- \ud83d\udccb **PROCEDURAL** = Instruction manual cabinet (steps to follow)\n- \u2764\ufe0f **EMOTIONAL** = Personal notes cabinet (preferences to honor)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory type classification\n",
    "class MemoryType(str, Enum):\n",
    "    \"\"\"Classification based on cognitive memory types.\"\"\"\n",
    "    SEMANTIC = \"semantic\"      # Facts, concepts\n",
    "    EPISODIC = \"episodic\"      # Events, experiences\n",
    "    PROCEDURAL = \"procedural\"  # How-to, patterns\n",
    "    EMOTIONAL = \"emotional\"    # Sentiment, preferences\n",
    "\n",
    "@dataclass\n",
    "class Memory:\n",
    "    \"\"\"Core memory unit with embedding and metadata.\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    embedding: np.ndarray\n",
    "    memory_type: MemoryType\n",
    "    \n",
    "    # Temporal tracking\n",
    "    created_at: float  # Unix timestamp\n",
    "    accessed_at: float\n",
    "    access_count: int = 0\n",
    "    \n",
    "    # Classification\n",
    "    hierarchy_path: str = \"\"  # e.g., \"tech/python/async\"\n",
    "    entity_ids: list[str] = field(default_factory=list)\n",
    "    \n",
    "    # Temperature (computed dynamically)\n",
    "    temperature: float = 0.5\n",
    "    \n",
    "    @property\n",
    "    def token_estimate(self) -> int:\n",
    "        \"\"\"Rough token count (~4 chars per token).\"\"\"\n",
    "        return len(self.content) // 4\n",
    "\n",
    "print(\"Memory types:\", [t.value for t in MemoryType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to create memories with deterministic mock embeddings\n",
    "def create_memory(\n",
    "    content: str,\n",
    "    memory_type: MemoryType = MemoryType.SEMANTIC,\n",
    "    hierarchy_path: str = \"\",\n",
    "    entity_ids: list[str] = None,\n",
    "    hours_ago: float = 0,\n",
    "    access_count: int = 1,\n",
    ") -> Memory:\n",
    "    \"\"\"\n",
    "    Create a memory with deterministic embedding based on content hash.\n",
    "    \n",
    "    In production, embeddings come from models like OpenAI's text-embedding-3-small.\n",
    "    Here we use content-based hashing for reproducible demos.\n",
    "    \"\"\"\n",
    "    now = utcnow().timestamp()\n",
    "    \n",
    "    # Deterministic embedding from content (384 dimensions like all-MiniLM-L6-v2)\n",
    "    np.random.seed(hash(content) % 2**32)\n",
    "    embedding = np.random.randn(384).astype(np.float32)\n",
    "    embedding = embedding / np.linalg.norm(embedding)  # Normalize to unit vector\n",
    "    \n",
    "    return Memory(\n",
    "        id=str(uuid.uuid4()),\n",
    "        content=content,\n",
    "        embedding=embedding,\n",
    "        memory_type=memory_type,\n",
    "        created_at=now - (hours_ago * 3600),\n",
    "        accessed_at=now - (hours_ago * 3600),\n",
    "        access_count=access_count,\n",
    "        hierarchy_path=hierarchy_path,\n",
    "        entity_ids=entity_ids or [],\n",
    "    )\n",
    "\n",
    "# Create sample memories for the workshop\n",
    "sample_memories = [\n",
    "    create_memory(\n",
    "        \"Python is a versatile programming language with dynamic typing and extensive libraries.\",\n",
    "        MemoryType.SEMANTIC,\n",
    "        \"tech/python\",\n",
    "        [\"python\", \"programming\"],\n",
    "        hours_ago=2,\n",
    "        access_count=5,\n",
    "    ),\n",
    "    create_memory(\n",
    "        \"We discussed the API design for the authentication module in yesterday's meeting.\",\n",
    "        MemoryType.EPISODIC,\n",
    "        \"projects/api\",\n",
    "        [\"api\", \"authentication\", \"meeting\"],\n",
    "        hours_ago=24,\n",
    "        access_count=2,\n",
    "    ),\n",
    "    create_memory(\n",
    "        \"To deploy the application: run `docker compose up -d` then `./scripts/migrate.sh`\",\n",
    "        MemoryType.PROCEDURAL,\n",
    "        \"ops/deployment\",\n",
    "        [\"docker\", \"deployment\", \"devops\"],\n",
    "        hours_ago=48,\n",
    "        access_count=10,\n",
    "    ),\n",
    "    create_memory(\n",
    "        \"User prefers concise, technical explanations over verbose ones.\",\n",
    "        MemoryType.EMOTIONAL,\n",
    "        \"preferences/style\",\n",
    "        [\"preference\", \"communication\"],\n",
    "        hours_ago=72,\n",
    "        access_count=3,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(sample_memories)} sample memories:\")\n",
    "for m in sample_memories:\n",
    "    print(f\"  [{m.memory_type.value:10}] {m.content[:50]}...\")\n",
    "    print(f\"                  Path: {m.hierarchy_path}, Entities: {m.entity_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 3: Agent State Management\n\n### The Concept\n\n**Agent State Management** is about maintaining the \"mental state\" of an AI agent across interactions. This includes:\n\n- **Focus** - What the agent is currently \"thinking about\" (embedding, entities, topic hierarchy)\n- **Session** - The current interaction period (turn count, timing)\n- **Episodes** - Coherent conversation segments grouped by topic\n\n### Why This Matters for AI Agents\n\nConsider a coding assistant:\n\n| Without State Management | With State Management |\n|-------------------------|----------------------|\n| Every message is isolated | Agent remembers you were debugging auth |\n| Repeats the same questions | Knows your preferences from past sessions |\n| Can't track topic changes | Detects when you switch from coding to deployment |\n| Generic responses | Personalized to your role and context |\n\n### Core Components\n\n#### 1. Focus Tracking (Context Engineering)\nThe agent's **focus** determines which memories are most relevant RIGHT NOW:\n- `focus_embedding` - Vector representation of current topic\n- `focus_entities` - Key concepts being discussed (e.g., {\"python\", \"async\", \"error handling\"})\n- `focus_hierarchy_path` - Position in knowledge hierarchy (e.g., \"tech/python/async\")\n\n#### 2. Episode Management\n**Episodes** are coherent segments of conversation. Detecting episode boundaries helps:\n- Summarize completed topics\n- Archive relevant context\n- Reset focus for new topics\n\n#### 3. Session Tracking\n- `turn_count` - How many interactions in this session\n- `last_interaction` - For timeout and recency calculations\n\n### What the Code Demonstrates\n\n1. **AgentContext** dataclass - Complete state for a single agent\n2. **Episode** dataclass - Conversation segment with topic summary\n3. **update_focus()** - How focus changes as conversation evolves\n4. **Demo** - Two agents (researcher, coder) with different contexts\n\n### Mental Model\n\nThink of an agent like a person in a meeting:\n- \ud83c\udfaf **Focus** = What they're currently paying attention to\n- \ud83d\udde3\ufe0f **Episode** = The current agenda item being discussed\n- \ud83d\udcca **Session** = The entire meeting\n- \ud83e\udde0 **Context** = Everything they know + their role"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Episode:\n",
    "    \"\"\"A coherent episode of interaction (like a conversation topic).\"\"\"\n",
    "    id: str\n",
    "    started_at: float\n",
    "    ended_at: float | None = None\n",
    "    topic_summary: str = \"\"\n",
    "    key_entities: list[str] = field(default_factory=list)\n",
    "    memory_ids: list[str] = field(default_factory=list)\n",
    "    turn_count: int = 0\n",
    "\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"Per-agent context for memory operations and focus tracking.\"\"\"\n",
    "    agent_id: str\n",
    "    agent_role: str  # \"researcher\", \"coder\", \"planner\", etc.\n",
    "    session_id: str\n",
    "    \n",
    "    # Focus tracking - what the agent is currently \"thinking about\"\n",
    "    focus_embedding: np.ndarray | None = None\n",
    "    focus_entities: set[str] = field(default_factory=set)\n",
    "    focus_hierarchy_path: str = \"\"\n",
    "    \n",
    "    # Session state\n",
    "    turn_count: int = 0\n",
    "    last_interaction: float = field(default_factory=lambda: utcnow().timestamp())\n",
    "    \n",
    "    # Episode tracking\n",
    "    current_episode: Episode | None = None\n",
    "    episode_history: list[Episode] = field(default_factory=list)\n",
    "    \n",
    "    def update_focus(\n",
    "        self,\n",
    "        embedding: np.ndarray | None = None,\n",
    "        entities: set[str] | None = None,\n",
    "        hierarchy_path: str | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Update the agent's current focus (context engineering).\"\"\"\n",
    "        if embedding is not None:\n",
    "            self.focus_embedding = embedding\n",
    "        if entities is not None:\n",
    "            self.focus_entities = entities\n",
    "        if hierarchy_path is not None:\n",
    "            self.focus_hierarchy_path = hierarchy_path\n",
    "        self.last_interaction = utcnow().timestamp()\n",
    "        self.turn_count += 1\n",
    "    \n",
    "    def start_episode(self, topic: str = \"\") -> Episode:\n",
    "        \"\"\"Start a new episode (topic segment).\"\"\"\n",
    "        if self.current_episode:\n",
    "            self.end_episode()\n",
    "        \n",
    "        self.current_episode = Episode(\n",
    "            id=str(uuid.uuid4()),\n",
    "            started_at=utcnow().timestamp(),\n",
    "            topic_summary=topic,\n",
    "            key_entities=list(self.focus_entities),\n",
    "        )\n",
    "        return self.current_episode\n",
    "    \n",
    "    def end_episode(self) -> Episode | None:\n",
    "        \"\"\"End current episode and archive it.\"\"\"\n",
    "        if not self.current_episode:\n",
    "            return None\n",
    "        \n",
    "        episode = self.current_episode\n",
    "        episode.ended_at = utcnow().timestamp()\n",
    "        self.episode_history.append(episode)\n",
    "        self.current_episode = None\n",
    "        return episode\n",
    "\n",
    "def create_agent(agent_id: str, role: str = \"general\") -> AgentContext:\n",
    "    \"\"\"Factory to create an agent context.\"\"\"\n",
    "    return AgentContext(\n",
    "        agent_id=agent_id,\n",
    "        agent_role=role,\n",
    "        session_id=str(uuid.uuid4()),\n",
    "    )\n",
    "\n",
    "print(\"AgentContext defined with focus tracking and episode management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Create two agents with different roles and contexts\n",
    "\n",
    "# Researcher agent focusing on ML\n",
    "researcher = create_agent(\"researcher-001\", \"researcher\")\n",
    "np.random.seed(42)\n",
    "ml_embedding = np.random.randn(384).astype(np.float32)\n",
    "ml_embedding = ml_embedding / np.linalg.norm(ml_embedding)\n",
    "\n",
    "researcher.update_focus(\n",
    "    embedding=ml_embedding,\n",
    "    entities={\"machine learning\", \"neural networks\", \"deep learning\"},\n",
    "    hierarchy_path=\"research/ai/ml\",\n",
    ")\n",
    "researcher.start_episode(\"Exploring ML architectures\")\n",
    "\n",
    "# Coder agent focusing on API development\n",
    "coder = create_agent(\"coder-001\", \"coder\")\n",
    "np.random.seed(123)\n",
    "api_embedding = np.random.randn(384).astype(np.float32)\n",
    "api_embedding = api_embedding / np.linalg.norm(api_embedding)\n",
    "\n",
    "coder.update_focus(\n",
    "    embedding=api_embedding,\n",
    "    entities={\"api\", \"authentication\", \"fastapi\"},\n",
    "    hierarchy_path=\"projects/api/auth\",\n",
    ")\n",
    "coder.start_episode(\"Implementing auth endpoints\")\n",
    "\n",
    "print(\"Agent States Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for agent in [researcher, coder]:\n",
    "    print(f\"\\n{agent.agent_role.upper()} ({agent.agent_id}):\")\n",
    "    print(f\"  Focus entities: {agent.focus_entities}\")\n",
    "    print(f\"  Focus path: {agent.focus_hierarchy_path}\")\n",
    "    print(f\"  Turn count: {agent.turn_count}\")\n",
    "    print(f\"  Current episode: {agent.current_episode.topic_summary}\")\n",
    "    print(f\"  Has focus embedding: {agent.focus_embedding is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 4: Temperature Scoring System\n\n### The Concept\n\n**Temperature** is a unified metric (0.0 to 1.0) that represents how \"hot\" or relevant a memory is RIGHT NOW. It combines multiple signals into a single score that determines:\n\n1. Which **tier** a memory belongs to (Hot, Warm, Cold)\n2. Whether to **promote** or **demote** a memory\n3. **Ranking** within search results\n\n### Why This Matters for AI Agents\n\nThe challenge: An agent might have 100,000 memories, but only ~4,000 tokens fit in context. How do you pick the RIGHT memories?\n\n**Naive approaches fail:**\n- \u274c Most recent? Misses relevant older knowledge\n- \u274c Most accessed? Ignores current context\n- \u274c Semantic similarity only? Misses entity connections\n\n**Temperature scoring succeeds by combining all signals:**\n- \u2705 Recent AND relevant AND frequently accessed = BLAZING hot\n- \u2705 Old but highly relevant to current query = Still gets promoted\n- \u2705 Recent but irrelevant = Cools down quickly\n\n### The Five Scoring Factors\n\n| Factor | Weight | What It Captures | How It's Calculated |\n|--------|--------|------------------|---------------------|\n| **Recency** | 30% | Time since last access | Exponential decay (24h half-life) |\n| **Frequency** | 15% | How often accessed | Access count / max_count |\n| **Relevance** | 35% | Semantic match to focus | Cosine similarity of embeddings |\n| **Entity Overlap** | 15% | Keyword/concept match | Jaccard-like intersection |\n| **Agent Match** | 5% | Same agent/role | Binary boost |\n\n### The Formula\n\n```\ntemperature = 0.30 \u00d7 recency + 0.15 \u00d7 frequency + 0.35 \u00d7 relevance + 0.15 \u00d7 entity_overlap + 0.05 \u00d7 agent_match\n```\n\n### Temperature Zones\n\n| Zone | Range | Meaning | Storage Tier |\n|------|-------|---------|--------------|\n| \ud83d\udd25 BLAZING | >0.85 | Critical for current task | Hot (in-memory) |\n| \ud83c\udf21\ufe0f HOT | 0.70-0.85 | Highly relevant, working memory | Hot (in-memory) |\n| \u2600\ufe0f WARM | 0.50-0.70 | Recently used, session cache | Warm (Redis) |\n| \ud83c\udf24\ufe0f COOLING | 0.30-0.50 | Fading relevance | Warm \u2192 Cold |\n| \u2744\ufe0f COLD | 0.10-0.30 | Archived, long-term storage | Cold (Postgres) |\n| \ud83e\uddca FROZEN | <0.10 | Deep archive, rarely accessed | Cold (archive) |\n\n### What the Code Demonstrates\n\n1. **TemperatureConfig** - Tunable weights and thresholds\n2. **TemperatureScorer** - The 5-factor computation\n3. **compute_breakdown()** - See contribution of each factor\n4. **Demo** - Same memories scored differently based on focus context\n\n### Mental Model\n\nThink of temperature like attention:\n- \ud83d\udd25 **Hot** = Top of mind, actively thinking about\n- \u2600\ufe0f **Warm** = In the back of your head, easily recalled\n- \u2744\ufe0f **Cold** = Stored away, requires effort to remember"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureZone(str, Enum):\n",
    "    \"\"\"Temperature zones for tier placement.\"\"\"\n",
    "    BLAZING = \"blazing\"   # >0.85 - Critical\n",
    "    HOT = \"hot\"           # 0.70-0.85 - Working memory\n",
    "    WARM = \"warm\"         # 0.50-0.70 - Session cache\n",
    "    COOLING = \"cooling\"   # 0.30-0.50 - Fading\n",
    "    COLD = \"cold\"         # 0.10-0.30 - Archived\n",
    "    FROZEN = \"frozen\"     # <0.10 - Deep archive\n",
    "\n",
    "@dataclass\n",
    "class TemperatureConfig:\n",
    "    \"\"\"Configuration for temperature computation.\"\"\"\n",
    "    # Component weights (must sum to 1.0)\n",
    "    weight_recency: float = 0.30\n",
    "    weight_frequency: float = 0.15\n",
    "    weight_relevance: float = 0.35\n",
    "    weight_entity: float = 0.15\n",
    "    weight_agent: float = 0.05\n",
    "    \n",
    "    # Decay parameters\n",
    "    recency_half_life_hours: float = 24.0  # Temperature halves every 24 hours\n",
    "    max_access_count: int = 100\n",
    "    \n",
    "    # Zone thresholds\n",
    "    blazing_threshold: float = 0.85\n",
    "    hot_threshold: float = 0.70\n",
    "    warm_threshold: float = 0.50\n",
    "    cooling_threshold: float = 0.30\n",
    "    cold_threshold: float = 0.10\n",
    "\n",
    "class TemperatureScorer:\n",
    "    \"\"\"Computes memory temperature for tier placement decisions.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TemperatureConfig | None = None):\n",
    "        self.config = config or TemperatureConfig()\n",
    "    \n",
    "    def compute(\n",
    "        self,\n",
    "        memory: Memory,\n",
    "        focus_embedding: np.ndarray | None = None,\n",
    "        focus_entities: set[str] | None = None,\n",
    "        current_time: float | None = None,\n",
    "    ) -> float:\n",
    "        \"\"\"Compute temperature for a memory based on 5 factors.\"\"\"\n",
    "        current_time = current_time or utcnow().timestamp()\n",
    "        \n",
    "        # 1. RECENCY: Exponential decay based on last access\n",
    "        hours_ago = (current_time - memory.accessed_at) / 3600.0\n",
    "        recency = np.power(0.5, hours_ago / self.config.recency_half_life_hours)\n",
    "        \n",
    "        # 2. FREQUENCY: How often is this memory accessed?\n",
    "        frequency = min(memory.access_count / self.config.max_access_count, 1.0)\n",
    "        \n",
    "        # 3. RELEVANCE: Cosine similarity to current focus\n",
    "        if focus_embedding is not None:\n",
    "            dot = np.dot(memory.embedding, focus_embedding)\n",
    "            norm_m = np.linalg.norm(memory.embedding)\n",
    "            norm_f = np.linalg.norm(focus_embedding)\n",
    "            # Map cosine similarity [-1, 1] to [0, 1]\n",
    "            relevance = float(np.clip((dot / (norm_m * norm_f + 1e-8) + 1) / 2, 0, 1))\n",
    "        else:\n",
    "            relevance = 0.5  # Default when no focus\n",
    "        \n",
    "        # 4. ENTITY OVERLAP: Jaccard-like intersection with focus entities\n",
    "        if focus_entities and memory.entity_ids:\n",
    "            memory_entities = set(memory.entity_ids)\n",
    "            intersection = len(memory_entities & focus_entities)\n",
    "            entity_score = intersection / len(focus_entities) if focus_entities else 0\n",
    "        else:\n",
    "            entity_score = 0.0\n",
    "        \n",
    "        # 5. AGENT MATCH: Would check agent_id match in full implementation\n",
    "        agent_score = 0.5  # Neutral for this demo\n",
    "        \n",
    "        # Weighted combination\n",
    "        temperature = (\n",
    "            self.config.weight_recency * recency +\n",
    "            self.config.weight_frequency * frequency +\n",
    "            self.config.weight_relevance * relevance +\n",
    "            self.config.weight_entity * entity_score +\n",
    "            self.config.weight_agent * agent_score\n",
    "        )\n",
    "        \n",
    "        return float(np.clip(temperature, 0.0, 1.0))\n",
    "    \n",
    "    def compute_breakdown(\n",
    "        self,\n",
    "        memory: Memory,\n",
    "        focus_embedding: np.ndarray | None = None,\n",
    "        focus_entities: set[str] | None = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"Compute temperature with full factor breakdown.\"\"\"\n",
    "        current_time = utcnow().timestamp()\n",
    "        hours_ago = (current_time - memory.accessed_at) / 3600.0\n",
    "        \n",
    "        recency = np.power(0.5, hours_ago / self.config.recency_half_life_hours)\n",
    "        frequency = min(memory.access_count / self.config.max_access_count, 1.0)\n",
    "        \n",
    "        if focus_embedding is not None:\n",
    "            dot = np.dot(memory.embedding, focus_embedding)\n",
    "            norm_m = np.linalg.norm(memory.embedding)\n",
    "            norm_f = np.linalg.norm(focus_embedding)\n",
    "            relevance = float(np.clip((dot / (norm_m * norm_f + 1e-8) + 1) / 2, 0, 1))\n",
    "        else:\n",
    "            relevance = 0.5\n",
    "        \n",
    "        if focus_entities and memory.entity_ids:\n",
    "            memory_entities = set(memory.entity_ids)\n",
    "            intersection = len(memory_entities & focus_entities)\n",
    "            entity_score = intersection / len(focus_entities)\n",
    "        else:\n",
    "            entity_score = 0.0\n",
    "        \n",
    "        return {\n",
    "            \"recency\": recency,\n",
    "            \"frequency\": frequency,\n",
    "            \"relevance\": relevance,\n",
    "            \"entity_overlap\": entity_score,\n",
    "            \"hours_ago\": hours_ago,\n",
    "        }\n",
    "    \n",
    "    def get_zone(self, temperature: float) -> TemperatureZone:\n",
    "        \"\"\"Get temperature zone for a value.\"\"\"\n",
    "        if temperature >= self.config.blazing_threshold:\n",
    "            return TemperatureZone.BLAZING\n",
    "        elif temperature >= self.config.hot_threshold:\n",
    "            return TemperatureZone.HOT\n",
    "        elif temperature >= self.config.warm_threshold:\n",
    "            return TemperatureZone.WARM\n",
    "        elif temperature >= self.config.cooling_threshold:\n",
    "            return TemperatureZone.COOLING\n",
    "        elif temperature >= self.config.cold_threshold:\n",
    "            return TemperatureZone.COLD\n",
    "        else:\n",
    "            return TemperatureZone.FROZEN\n",
    "    \n",
    "    def should_promote_to_hot(self, temperature: float) -> bool:\n",
    "        \"\"\"Should this memory be promoted to hot tier?\"\"\"\n",
    "        return temperature >= self.config.hot_threshold\n",
    "    \n",
    "    def should_demote_from_hot(self, temperature: float) -> bool:\n",
    "        \"\"\"Should this memory be demoted from hot tier?\"\"\"\n",
    "        return temperature < self.config.warm_threshold\n",
    "\n",
    "# Create global scorer\n",
    "scorer = TemperatureScorer()\n",
    "print(\"TemperatureScorer ready!\")\n",
    "print(f\"Weights: recency={scorer.config.weight_recency}, frequency={scorer.config.weight_frequency}, \"\n",
    "      f\"relevance={scorer.config.weight_relevance}, entity={scorer.config.weight_entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Score memories with different focus contexts\n",
    "\n",
    "# Create focus embedding for \"Python programming\"\n",
    "np.random.seed(hash(\"Python programming\") % 2**32)\n",
    "python_focus = np.random.randn(384).astype(np.float32)\n",
    "python_focus = python_focus / np.linalg.norm(python_focus)\n",
    "python_entities = {\"python\", \"programming\", \"code\"}\n",
    "\n",
    "print(\"Temperature Scoring Demo\")\n",
    "print(f\"Focus: 'Python programming'\")\n",
    "print(f\"Focus entities: {python_entities}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for memory in sample_memories:\n",
    "    temp = scorer.compute(\n",
    "        memory=memory,\n",
    "        focus_embedding=python_focus,\n",
    "        focus_entities=python_entities,\n",
    "    )\n",
    "    breakdown = scorer.compute_breakdown(\n",
    "        memory=memory,\n",
    "        focus_embedding=python_focus,\n",
    "        focus_entities=python_entities,\n",
    "    )\n",
    "    zone = scorer.get_zone(temp)\n",
    "    promote = scorer.should_promote_to_hot(temp)\n",
    "    \n",
    "    # Update memory's temperature\n",
    "    memory.temperature = temp\n",
    "    \n",
    "    print(f\"\\n[{memory.memory_type.value.upper():10}] Temperature: {temp:.3f} | Zone: {zone.value:8}\")\n",
    "    print(f\"  Content: {memory.content[:50]}...\")\n",
    "    print(f\"  Breakdown:\")\n",
    "    print(f\"    Recency:  {breakdown['recency']:.3f} ({breakdown['hours_ago']:.1f}h ago)\")\n",
    "    print(f\"    Frequency: {breakdown['frequency']:.3f} ({memory.access_count} accesses)\")\n",
    "    print(f\"    Relevance: {breakdown['relevance']:.3f}\")\n",
    "    print(f\"    Entity:    {breakdown['entity_overlap']:.3f} (overlap: {set(memory.entity_ids) & python_entities})\")\n",
    "    print(f\"  -> Promote to HOT: {promote}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 5: Hot Tier (Working Memory)\n\n### The Concept\n\nThe **Hot Tier** is the agent's \"working memory\"\u2014the fastest, most expensive tier designed for sub-millisecond access to the most critical memories. In production, this is implemented in Rust with SIMD acceleration.\n\n### Why This Matters for AI Agents\n\nLLM context windows are limited (4K-128K tokens). The hot tier ensures:\n\n| Constraint | Hot Tier Solution |\n|------------|-------------------|\n| Limited context window | **Token budget** - Stays within limits automatically |\n| Slow retrieval = slow responses | **<1ms access** - No perceptible delay |\n| Irrelevant context = poor answers | **LRU eviction** - Old/unused memories make room for new |\n| Need fast similarity search | **SIMD vectorization** - Batch cosine similarity in NumPy |\n\n### Key Features\n\n#### 1. Token-Based Eviction\nUnlike traditional caches that evict by count, the hot tier evicts by **token count**:\n- Max budget: ~8,000 tokens (configurable)\n- When full: Evict **Least Recently Used** memories\n- Why tokens? LLM context is measured in tokens, not memory count\n\n#### 2. Vectorized Similarity Search\nFor fast retrieval, we compute similarity against ALL hot memories at once:\n```python\n# Batch cosine similarity (vectorized)\nsimilarities = (embeddings / norms) @ query_vector\n```\nThis is O(n) but with very small constants due to SIMD parallelism.\n\n#### 3. Entity Boost\nMemories matching the current focus entities get a relevance boost:\n- +10% per matching entity\n- Max +30% boost\n- Why? Keywords indicate topical relevance beyond embeddings\n\n### Hot Tier vs Traditional Cache\n\n| Traditional Cache | Hot Tier |\n|------------------|----------|\n| Key-value lookup | Similarity search |\n| Fixed-size entries | Variable-size (tokens) |\n| Count-based eviction | Token-budget eviction |\n| Binary hit/miss | Relevance-ranked results |\n\n### What the Code Demonstrates\n\n1. **HotMemory** - Optimized dataclass for hot tier storage\n2. **HotTier.put()** - Add with automatic LRU eviction\n3. **HotTier.scan()** - Vectorized similarity search with entity boost\n4. **Demo** - Add memories, observe eviction, scan with query\n\n### Mental Model\n\nThink of the hot tier like a whiteboard in a meeting:\n- \ud83d\udcdd **Limited space** - Can only fit so much\n- \ud83d\udd04 **Erase old stuff** - Make room for new relevant info\n- \u26a1 **Instantly visible** - Everyone can see it immediately\n- \ud83c\udfaf **Most important items** - Only key points go on the board"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HotMemory:\n",
    "    \"\"\"Hot tier memory - optimized for fast access.\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    embedding: np.ndarray\n",
    "    memory_type: MemoryType\n",
    "    \n",
    "    accessed_at: float\n",
    "    access_count: int\n",
    "    entity_ids: list[str]\n",
    "    hierarchy_path: str\n",
    "    token_count: int\n",
    "    temperature: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class ScanResult:\n",
    "    \"\"\"Result from hot tier scan.\"\"\"\n",
    "    memories: list[HotMemory]\n",
    "    scores: list[float]\n",
    "    total_scanned: int\n",
    "    scan_time_ms: float\n",
    "\n",
    "class HotTier:\n",
    "    \"\"\"Working memory tier using in-memory storage with LRU eviction.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str, max_tokens: int = 8000):\n",
    "        self.agent_id = agent_id\n",
    "        self.max_tokens = max_tokens\n",
    "        self._memories: dict[str, HotMemory] = {}\n",
    "        self._token_count = 0\n",
    "        self._access_order: dict[str, float] = {}  # LRU tracking\n",
    "        self._scan_count = 0\n",
    "        self._eviction_count = 0\n",
    "    \n",
    "    def put(self, memory: HotMemory) -> list[str]:\n",
    "        \"\"\"Add memory, evicting LRU if over token budget. Returns evicted IDs.\"\"\"\n",
    "        evicted = []\n",
    "        \n",
    "        # Evict until we have room\n",
    "        while (self._token_count + memory.token_count > self.max_tokens \n",
    "               and self._memories):\n",
    "            evicted_id = self._evict_lru()\n",
    "            if evicted_id:\n",
    "                evicted.append(evicted_id)\n",
    "        \n",
    "        self._memories[memory.id] = memory\n",
    "        self._token_count += memory.token_count\n",
    "        self._access_order[memory.id] = utcnow().timestamp()\n",
    "        \n",
    "        return evicted\n",
    "    \n",
    "    def get(self, memory_id: str) -> HotMemory | None:\n",
    "        \"\"\"Get memory by ID and update access time.\"\"\"\n",
    "        memory = self._memories.get(memory_id)\n",
    "        if memory:\n",
    "            self._access_order[memory_id] = utcnow().timestamp()\n",
    "            memory.access_count += 1\n",
    "        return memory\n",
    "    \n",
    "    def scan(\n",
    "        self,\n",
    "        query_embedding: np.ndarray,\n",
    "        focus_entities: set[str] | None = None,\n",
    "        limit: int = 10,\n",
    "        min_similarity: float = 0.0,\n",
    "    ) -> ScanResult:\n",
    "        \"\"\"Scan hot tier for relevant memories using vectorized similarity.\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        self._scan_count += 1\n",
    "        \n",
    "        if not self._memories:\n",
    "            return ScanResult([], [], 0, 0.0)\n",
    "        \n",
    "        memory_list = list(self._memories.values())\n",
    "        \n",
    "        # Batch cosine similarity (vectorized for speed)\n",
    "        embeddings = np.array([m.embedding for m in memory_list])\n",
    "        query_norm = query_embedding / (np.linalg.norm(query_embedding) + 1e-8)\n",
    "        mem_norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8\n",
    "        similarities = (embeddings / mem_norms) @ query_norm\n",
    "        \n",
    "        # Entity boost: +10% per matching entity, max +30%\n",
    "        if focus_entities:\n",
    "            for i, mem in enumerate(memory_list):\n",
    "                overlap = len(set(mem.entity_ids) & focus_entities)\n",
    "                if overlap > 0:\n",
    "                    boost = min(0.3, overlap * 0.1)\n",
    "                    similarities[i] = min(1.0, similarities[i] * (1 + boost))\n",
    "        \n",
    "        # Combine similarity with temperature (70% similarity + 30% temperature)\n",
    "        final_scores = [\n",
    "            0.7 * sim + 0.3 * mem.temperature\n",
    "            for sim, mem in zip(similarities, memory_list)\n",
    "        ]\n",
    "        \n",
    "        # Filter and sort\n",
    "        scored = [\n",
    "            (mem, score, sim)\n",
    "            for mem, score, sim in zip(memory_list, final_scores, similarities)\n",
    "            if sim >= min_similarity\n",
    "        ]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        top = scored[:limit]\n",
    "        \n",
    "        elapsed_ms = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        return ScanResult(\n",
    "            memories=[m for m, _, _ in top],\n",
    "            scores=[s for _, s, _ in top],\n",
    "            total_scanned=len(memory_list),\n",
    "            scan_time_ms=elapsed_ms,\n",
    "        )\n",
    "    \n",
    "    def _evict_lru(self) -> str | None:\n",
    "        \"\"\"Evict least recently used memory.\"\"\"\n",
    "        if not self._access_order:\n",
    "            return None\n",
    "        lru_id = min(self._access_order, key=self._access_order.get)\n",
    "        memory = self._memories.pop(lru_id, None)\n",
    "        if memory:\n",
    "            self._token_count -= memory.token_count\n",
    "            self._access_order.pop(lru_id)\n",
    "            self._eviction_count += 1\n",
    "        return lru_id\n",
    "    \n",
    "    def stats(self) -> dict:\n",
    "        return {\n",
    "            \"memory_count\": len(self._memories),\n",
    "            \"token_count\": self._token_count,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"utilization\": f\"{self._token_count / self.max_tokens:.1%}\",\n",
    "            \"scan_count\": self._scan_count,\n",
    "            \"eviction_count\": self._eviction_count,\n",
    "        }\n",
    "\n",
    "print(\"HotTier defined with LRU eviction and vectorized similarity search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Hot tier operations\n",
    "\n",
    "# Helper to convert Memory to HotMemory\n",
    "def to_hot_memory(m: Memory) -> HotMemory:\n",
    "    return HotMemory(\n",
    "        id=m.id,\n",
    "        content=m.content,\n",
    "        embedding=m.embedding,\n",
    "        memory_type=m.memory_type,\n",
    "        accessed_at=m.accessed_at,\n",
    "        access_count=m.access_count,\n",
    "        entity_ids=m.entity_ids,\n",
    "        hierarchy_path=m.hierarchy_path,\n",
    "        token_count=m.token_estimate,\n",
    "        temperature=m.temperature,\n",
    "    )\n",
    "\n",
    "# Create hot tier with small budget to show eviction\n",
    "hot = HotTier(agent_id=\"demo-agent\", max_tokens=500)\n",
    "\n",
    "print(\"Adding memories to Hot Tier (max 500 tokens):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for m in sample_memories:\n",
    "    hot_mem = to_hot_memory(m)\n",
    "    evicted = hot.put(hot_mem)\n",
    "    print(f\"Added: {m.content[:40]}... ({hot_mem.token_count} tokens)\")\n",
    "    if evicted:\n",
    "        print(f\"  -> Evicted {len(evicted)} memory(ies) to make room\")\n",
    "\n",
    "print(f\"\\nHot Tier Stats: {hot.stats()}\")\n",
    "\n",
    "# Scan hot tier\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Scanning Hot Tier for: 'Python programming'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = hot.scan(\n",
    "    query_embedding=python_focus,\n",
    "    focus_entities={\"python\", \"programming\"},\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "print(f\"\\nScan time: {result.scan_time_ms:.3f}ms\")\n",
    "print(f\"Scanned: {result.total_scanned} memories\")\n",
    "print(\"\\nResults:\")\n",
    "for mem, score in zip(result.memories, result.scores):\n",
    "    print(f\"  [{score:.3f}] {mem.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 6: Agentic RAG - Active Recall with Promotion\n\n### The Concept\n\n**Agentic RAG** (Retrieval-Augmented Generation) goes beyond passive document retrieval. Instead of just fetching relevant content, the system **actively reorganizes** its memory based on what's being accessed.\n\nKey behaviors:\n- **Promotion**: Frequently accessed cold memories get promoted to warmer tiers\n- **Demotion**: Unused hot memories cool down and get demoted\n- **Self-Organization**: The system learns what's important through usage patterns\n\n### Why This Matters for AI Agents\n\nTraditional RAG is **static**\u2014the same query always searches the same indexes. Agentic RAG is **dynamic**:\n\n| Traditional RAG | Agentic RAG |\n|----------------|-------------|\n| Search once, return results | Search + reorganize for next time |\n| All memories equally accessible | Hot memories are faster to access |\n| No learning from access patterns | Frequently used = promoted |\n| Cold start every query | Warmed-up context from recent queries |\n\n### The Three-Tier Architecture\n\n```\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502    HOT TIER     \u2502  \u2190 <1ms access (Rust in-memory)\n         \u2502   Working Set   \u2502     Most relevant RIGHT NOW\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2 promote \u2502 demote \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502    WARM TIER    \u2502  \u2190 <50ms access (Redis)\n         \u2502  Session Cache  \u2502     Recently accessed\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2 promote \u2502 demote \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502    COLD TIER    \u2502  \u2190 <200ms access (PostgreSQL)\n         \u2502   Long-term     \u2502     Full knowledge base\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Tier Movement Rules\n\n| Current Tier | Temperature | Action |\n|--------------|-------------|--------|\n| Cold | \u22650.70 | **Promote to Hot** |\n| Cold | \u22650.50 | Promote to Warm |\n| Warm | \u22650.70 | **Promote to Hot** |\n| Warm | <0.50 | Demote to Cold |\n| Hot | <0.50 | **Demote to Warm** |\n\n### The Recall Flow\n\n1. **Query arrives** \u2192 Compute query embedding\n2. **Search all tiers** in parallel (Hot, Warm, Cold)\n3. **Score each result** using temperature formula\n4. **Promote/demote** based on new temperatures\n5. **Return ranked results** with hottest first\n\n### What the Code Demonstrates\n\n1. **ThreeTierMemory** - Simulated three-tier system\n2. **recall()** - Active recall with automatic tier movement\n3. **TierPlacement** - Tracks promotions and demotions\n4. **Demo** - Query triggers promotion of relevant memories\n\n### Mental Model\n\nThink of Agentic RAG like a library with a reading desk:\n- \ud83d\udcda **Cold** = Books on the shelves (all knowledge)\n- \ud83d\udcd6 **Warm** = Books you've pulled out recently (session)\n- \ud83d\udcdd **Hot** = Open books on your desk (working set)\n\nWhen you reference a book often, it stays on your desk. When you stop using it, it goes back to the shelf."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TierPlacement:\n",
    "    \"\"\"Result of tier placement decision.\"\"\"\n",
    "    memory_id: str\n",
    "    source_tier: str\n",
    "    target_tier: str\n",
    "    temperature: float\n",
    "    promoted: bool\n",
    "    demoted: bool\n",
    "\n",
    "class ThreeTierMemory:\n",
    "    \"\"\"Simulated three-tier memory system with automatic promotion/demotion.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str):\n",
    "        self.agent_id = agent_id\n",
    "        self.scorer = TemperatureScorer()\n",
    "        \n",
    "        # Three tiers (in production: Hot=Rust memory, Warm=Redis, Cold=Postgres)\n",
    "        self.hot: dict[str, Memory] = {}\n",
    "        self.warm: dict[str, Memory] = {}\n",
    "        self.cold: dict[str, Memory] = {}\n",
    "        \n",
    "        # Thresholds for tier movement\n",
    "        self.hot_threshold = 0.70\n",
    "        self.warm_threshold = 0.50\n",
    "        self.cold_threshold = 0.30\n",
    "    \n",
    "    def store(self, memory: Memory) -> str:\n",
    "        \"\"\"Store memory - new memories start in warm tier.\"\"\"\n",
    "        self.warm[memory.id] = memory\n",
    "        return \"warm\"\n",
    "    \n",
    "    def recall(\n",
    "        self,\n",
    "        query_embedding: np.ndarray,\n",
    "        focus_entities: set[str],\n",
    "        limit: int = 10,\n",
    "    ) -> tuple[list[Memory], list[TierPlacement]]:\n",
    "        \"\"\"\n",
    "        Active recall with automatic tier promotion/demotion.\n",
    "        \n",
    "        Returns:\n",
    "            - List of recalled memories (sorted by relevance)\n",
    "            - List of tier placements (promotions/demotions)\n",
    "        \"\"\"\n",
    "        all_memories = []\n",
    "        placements = []\n",
    "        \n",
    "        # Collect all memories with their source tier\n",
    "        for tier_name, tier in [(\"hot\", self.hot), (\"warm\", self.warm), (\"cold\", self.cold)]:\n",
    "            for mem in tier.values():\n",
    "                mem._source_tier = tier_name\n",
    "                all_memories.append(mem)\n",
    "        \n",
    "        # Score all memories\n",
    "        scored = []\n",
    "        for mem in all_memories:\n",
    "            temp = self.scorer.compute(\n",
    "                memory=mem,\n",
    "                focus_embedding=query_embedding,\n",
    "                focus_entities=focus_entities,\n",
    "            )\n",
    "            mem.temperature = temp\n",
    "            scored.append((mem, temp))\n",
    "        \n",
    "        # Sort by temperature (relevance)\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_memories = [m for m, _ in scored[:limit]]\n",
    "        \n",
    "        # Handle tier movements\n",
    "        for mem, temp in scored:\n",
    "            source = getattr(mem, '_source_tier', 'cold')\n",
    "            target = self._get_target_tier(temp)\n",
    "            \n",
    "            if source != target:\n",
    "                self._move_memory(mem, source, target)\n",
    "                placements.append(TierPlacement(\n",
    "                    memory_id=mem.id,\n",
    "                    source_tier=source,\n",
    "                    target_tier=target,\n",
    "                    temperature=temp,\n",
    "                    promoted=(target == \"hot\" and source != \"hot\") or \n",
    "                             (target == \"warm\" and source == \"cold\"),\n",
    "                    demoted=(target == \"cold\" and source != \"cold\") or \n",
    "                            (target == \"warm\" and source == \"hot\"),\n",
    "                ))\n",
    "        \n",
    "        return top_memories, placements\n",
    "    \n",
    "    def _get_target_tier(self, temperature: float) -> str:\n",
    "        if temperature >= self.hot_threshold:\n",
    "            return \"hot\"\n",
    "        elif temperature >= self.warm_threshold:\n",
    "            return \"warm\"\n",
    "        else:\n",
    "            return \"cold\"\n",
    "    \n",
    "    def _move_memory(self, memory: Memory, source: str, target: str):\n",
    "        \"\"\"Move memory between tiers.\"\"\"\n",
    "        tiers = {\"hot\": self.hot, \"warm\": self.warm, \"cold\": self.cold}\n",
    "        tiers[source].pop(memory.id, None)\n",
    "        tiers[target][memory.id] = memory\n",
    "    \n",
    "    def stats(self) -> dict:\n",
    "        return {\n",
    "            \"hot\": len(self.hot),\n",
    "            \"warm\": len(self.warm),\n",
    "            \"cold\": len(self.cold),\n",
    "            \"total\": len(self.hot) + len(self.warm) + len(self.cold),\n",
    "        }\n",
    "\n",
    "print(\"ThreeTierMemory with active recall and promotion defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Active recall with tier promotion\n",
    "\n",
    "tiers = ThreeTierMemory(agent_id=\"demo-agent\")\n",
    "\n",
    "# Store sample memories (all start in warm tier)\n",
    "print(\"Storing memories (all start in WARM tier):\")\n",
    "print(\"-\" * 60)\n",
    "for m in sample_memories:\n",
    "    tier = tiers.store(m)\n",
    "    print(f\"  [{tier}] {m.content[:50]}...\")\n",
    "\n",
    "print(f\"\\nInitial tier distribution: {tiers.stats()}\")\n",
    "\n",
    "# Simulate recall with Python focus\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ACTIVE RECALL: 'Python programming' query\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "recalled, placements = tiers.recall(\n",
    "    query_embedding=python_focus,\n",
    "    focus_entities={\"python\", \"programming\", \"code\"},\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "print(f\"\\nRecalled {len(recalled)} memories (sorted by relevance):\")\n",
    "for m in recalled:\n",
    "    zone = scorer.get_zone(m.temperature)\n",
    "    print(f\"  [{m.temperature:.3f}] [{zone.value:8}] {m.content[:45]}...\")\n",
    "\n",
    "print(f\"\\nTier movements ({len(placements)} changes):\")\n",
    "for p in placements:\n",
    "    direction = \"PROMOTED\" if p.promoted else (\"DEMOTED\" if p.demoted else \"MOVED\")\n",
    "    print(f\"  {direction}: {p.source_tier} -> {p.target_tier} (temp: {p.temperature:.3f})\")\n",
    "\n",
    "print(f\"\\nFinal tier distribution: {tiers.stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 7: Pattern Graph System\n\n### The Concept\n\nThe **Pattern Graph** is a knowledge structure that connects **entities** (keywords, concepts) to **patterns** (learned response strategies). Unlike vector search alone, it enables:\n\n- **Graph-based retrieval**: Find patterns via connected keywords\n- **Semantic expansion**: \"eigenvalues\" connects to \"linear algebra\" patterns\n- **Fast keyword matching**: O(1) lookup by entity\n\n### Why This Matters for AI Agents\n\nEmbedding-based search finds semantically similar content, but misses explicit keyword connections:\n\n| Query | Embedding Search | Pattern Graph |\n|-------|------------------|---------------|\n| \"deploy\" | Finds similar deployment docs | Finds exact \"deploy\" pattern with known strategy |\n| \"JWT auth\" | Finds auth-related content | Finds specific JWT pattern via both keywords |\n| \"eigenvalues\" | Finds math content | Follows graph: eigenvalues \u2192 linear algebra \u2192 patterns |\n\n### Graph Structure\n\n```\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  ENTITIES  \u2502  \u2190 Keywords extracted from queries/patterns\n         \u2502 (keywords) \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502 edges (strength weighted)\n               \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  PATTERNS  \u2502  \u2190 Trigger + Strategy pairs\n         \u2502 (responses)\u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Algorithms Used\n\n#### 1. Keyword Extraction\n```python\ndef extract_keywords(text):\n    words = re.findall(r\"[a-zA-Z][a-zA-Z0-9_]*\", text.lower())\n    return [w for w in words if w not in STOPWORDS and len(w) >= 3]\n```\n- **Regex tokenization**: Extract word-like tokens\n- **Stopword filtering**: Remove common words (the, is, what, how, etc.)\n- **Length threshold**: Keep only words \u22653 characters\n\n#### 2. Entity-Pattern Linking\nEach entity connects to patterns with a **strength** weight (0.0-1.0):\n- Longer/rarer keywords get higher strength (0.9)\n- Common keywords get lower strength (0.7)\n- Strength = confidence that this keyword indicates the pattern\n\n#### 3. Graph Traversal for Pattern Matching\n```python\ndef find_patterns(keywords):\n    results = {}\n    for keyword in keywords:\n        if keyword in entity_to_patterns:\n            for (pattern_id, strength) in entity_to_patterns[keyword]:\n                score = strength \u00d7 pattern.effectiveness\n                results[pattern_id] = max(results.get(pattern_id), score)\n    return sorted(results, key=score, reverse=True)\n```\n- **O(k \u00d7 p)** where k=keywords, p=average patterns per keyword\n- **Aggregation**: Same pattern found via multiple keywords keeps max score\n- **Ranking**: Patterns sorted by score\n\n### What the Code Demonstrates\n\n1. **PatternGraph** - Entity-pattern graph structure\n2. **extract_keywords()** - Tokenization with stopword filtering\n3. **link_entity_to_pattern()** - Creating weighted edges\n4. **find_patterns()** - Graph traversal for pattern retrieval\n5. **Demo** - Build graph, query with different inputs\n\n### Mental Model\n\nThink of the pattern graph like a library card catalog:\n- \ud83d\udcc7 **Entity cards** = Subject index cards (keywords)\n- \ud83d\udcd8 **Pattern cards** = Book location cards (strategies)\n- \ud83d\udd17 **Links** = \"See also\" references between cards\n- \ud83d\udd0d **Search** = Follow index cards to find relevant books"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PatternMatch:\n",
    "    \"\"\"Result of a pattern search.\"\"\"\n",
    "    pattern_id: str\n",
    "    trigger: str\n",
    "    strategy: str\n",
    "    effectiveness: float\n",
    "    relevance: float\n",
    "    matched_via: list[str] = field(default_factory=list)\n",
    "\n",
    "class PatternGraph:\n",
    "    \"\"\"Entity-pattern graph for intelligent response routing.\"\"\"\n",
    "    \n",
    "    # Stopwords to filter during keyword extraction\n",
    "    STOPWORDS = frozenset({\n",
    "        # Articles, pronouns\n",
    "        \"a\", \"an\", \"the\", \"i\", \"me\", \"my\", \"we\", \"you\", \"your\", \"it\", \"this\", \"that\",\n",
    "        # Common verbs\n",
    "        \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"have\", \"has\", \"had\",\n",
    "        \"do\", \"does\", \"did\", \"will\", \"would\", \"can\", \"could\", \"should\",\n",
    "        # Prepositions, conjunctions\n",
    "        \"to\", \"of\", \"in\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\",\n",
    "        \"and\", \"but\", \"or\", \"if\", \"as\", \"so\",\n",
    "        # Question words\n",
    "        \"what\", \"how\", \"why\", \"when\", \"where\", \"who\", \"which\",\n",
    "        # Action words\n",
    "        \"let\", \"lets\", \"about\", \"tell\", \"explain\", \"help\", \"show\", \"make\",\n",
    "    })\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._entities: dict[str, int] = {}  # entity -> access count\n",
    "        self._patterns: dict[str, dict] = {}  # pattern_id -> pattern data\n",
    "        self._entity_to_patterns: dict[str, list[tuple[str, float]]] = {}  # entity -> [(pattern_id, strength)]\n",
    "        self._query_count = 0\n",
    "    \n",
    "    def add_entity(self, name: str) -> None:\n",
    "        \"\"\"Add an entity node.\"\"\"\n",
    "        name_lower = name.lower()\n",
    "        if name_lower not in self._entities:\n",
    "            self._entities[name_lower] = 0\n",
    "        self._entities[name_lower] += 1\n",
    "    \n",
    "    def add_pattern(\n",
    "        self,\n",
    "        pattern_id: str,\n",
    "        trigger: str,\n",
    "        strategy: str,\n",
    "        effectiveness: float = 0.7,\n",
    "    ) -> None:\n",
    "        \"\"\"Add a pattern node.\"\"\"\n",
    "        self._patterns[pattern_id] = {\n",
    "            \"id\": pattern_id,\n",
    "            \"trigger\": trigger,\n",
    "            \"strategy\": strategy,\n",
    "            \"effectiveness\": effectiveness,\n",
    "        }\n",
    "    \n",
    "    def link_entity_to_pattern(\n",
    "        self,\n",
    "        entity: str,\n",
    "        pattern_id: str,\n",
    "        strength: float = 0.8,\n",
    "    ) -> None:\n",
    "        \"\"\"Link an entity to a pattern with given strength.\"\"\"\n",
    "        entity_lower = entity.lower()\n",
    "        if entity_lower not in self._entity_to_patterns:\n",
    "            self._entity_to_patterns[entity_lower] = []\n",
    "        self._entity_to_patterns[entity_lower].append((pattern_id, strength))\n",
    "    \n",
    "    def find_patterns(\n",
    "        self,\n",
    "        keywords: list[str],\n",
    "        limit: int = 10,\n",
    "    ) -> list[PatternMatch]:\n",
    "        \"\"\"Find patterns matching keywords via graph traversal.\"\"\"\n",
    "        self._query_count += 1\n",
    "        results: dict[str, PatternMatch] = {}\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            kw_lower = keyword.lower()\n",
    "            if kw_lower in self._entity_to_patterns:\n",
    "                for pattern_id, strength in self._entity_to_patterns[kw_lower]:\n",
    "                    if pattern_id in self._patterns:\n",
    "                        pattern = self._patterns[pattern_id]\n",
    "                        score = strength * pattern[\"effectiveness\"]\n",
    "                        \n",
    "                        if pattern_id not in results or score > results[pattern_id].relevance:\n",
    "                            results[pattern_id] = PatternMatch(\n",
    "                                pattern_id=pattern_id,\n",
    "                                trigger=pattern[\"trigger\"],\n",
    "                                strategy=pattern[\"strategy\"],\n",
    "                                effectiveness=pattern[\"effectiveness\"],\n",
    "                                relevance=score,\n",
    "                                matched_via=[kw_lower],\n",
    "                            )\n",
    "        \n",
    "        sorted_results = sorted(results.values(), key=lambda m: -m.relevance)\n",
    "        return sorted_results[:limit]\n",
    "    \n",
    "    def extract_keywords(self, text: str) -> list[str]:\n",
    "        \"\"\"Extract meaningful keywords from text.\"\"\"\n",
    "        words = re.findall(r\"[a-zA-Z][a-zA-Z0-9_]*\", text.lower())\n",
    "        return [w for w in words if w not in self.STOPWORDS and len(w) >= 3]\n",
    "    \n",
    "    def stats(self) -> dict:\n",
    "        return {\n",
    "            \"entities\": len(self._entities),\n",
    "            \"patterns\": len(self._patterns),\n",
    "            \"edges\": sum(len(v) for v in self._entity_to_patterns.values()),\n",
    "            \"queries\": self._query_count,\n",
    "        }\n",
    "\n",
    "print(\"PatternGraph defined with entity-pattern linking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pattern graph with sample patterns\n",
    "\n",
    "graph = PatternGraph()\n",
    "\n",
    "# Define patterns (simulating learned response strategies)\n",
    "patterns = [\n",
    "    {\n",
    "        \"id\": \"pat_deploy\",\n",
    "        \"trigger\": \"How to deploy the application?\",\n",
    "        \"strategy\": \"Use docker compose: `docker compose up -d && ./scripts/migrate.sh`\",\n",
    "        \"keywords\": [\"deploy\", \"deployment\", \"docker\", \"application\", \"production\"],\n",
    "        \"effectiveness\": 0.9,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pat_auth\",\n",
    "        \"trigger\": \"How does authentication work?\",\n",
    "        \"strategy\": \"JWT tokens via /api/v1/auth/login endpoint. Tokens expire in 24h.\",\n",
    "        \"keywords\": [\"authentication\", \"auth\", \"login\", \"jwt\", \"token\", \"security\"],\n",
    "        \"effectiveness\": 0.85,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pat_python\",\n",
    "        \"trigger\": \"What is Python?\",\n",
    "        \"strategy\": \"Python is a versatile, dynamically-typed language popular for web, data science, and automation.\",\n",
    "        \"keywords\": [\"python\", \"programming\", \"language\", \"coding\"],\n",
    "        \"effectiveness\": 0.8,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pat_ml\",\n",
    "        \"trigger\": \"How to train a neural network?\",\n",
    "        \"strategy\": \"Define model architecture, prepare data, set loss function and optimizer, iterate through epochs.\",\n",
    "        \"keywords\": [\"neural\", \"network\", \"train\", \"machine\", \"learning\", \"deep\", \"model\"],\n",
    "        \"effectiveness\": 0.75,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Building Pattern Graph:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for p in patterns:\n",
    "    graph.add_pattern(p[\"id\"], p[\"trigger\"], p[\"strategy\"], p[\"effectiveness\"])\n",
    "    for kw in p[\"keywords\"]:\n",
    "        graph.add_entity(kw)\n",
    "        # Longer keywords get higher strength\n",
    "        strength = 0.9 if len(kw) > 4 else 0.7\n",
    "        graph.link_entity_to_pattern(kw, p[\"id\"], strength)\n",
    "    print(f\"  Added: {p['trigger'][:40]}... ({len(p['keywords'])} keywords)\")\n",
    "\n",
    "print(f\"\\nGraph stats: {graph.stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query pattern graph with various queries\n",
    "\n",
    "test_queries = [\n",
    "    \"How do I deploy my app to production?\",\n",
    "    \"Tell me about Python programming\",\n",
    "    \"JWT authentication setup\",\n",
    "    \"deep learning training process\",\n",
    "    \"What's the weather like?\",  # No match expected\n",
    "]\n",
    "\n",
    "print(\"Pattern Graph Queries\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in test_queries:\n",
    "    keywords = graph.extract_keywords(query)\n",
    "    matches = graph.find_patterns(keywords, limit=3)\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Keywords: {keywords}\")\n",
    "    \n",
    "    if matches:\n",
    "        for m in matches:\n",
    "            print(f\"  [{m.relevance:.3f}] {m.trigger[:45]}...\")\n",
    "            print(f\"           Strategy: {m.strategy[:50]}...\")\n",
    "    else:\n",
    "        print(\"  No patterns found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 8: Pattern Relevance Scoring\n\n### The Concept\n\nOnce patterns are retrieved, we need to **rank** them by relevance to the query. **Pattern Scoring** combines multiple signals to produce a robust relevance score that's better than any single signal alone.\n\n### Why This Matters for AI Agents\n\nEach scoring signal has weaknesses:\n\n| Signal Alone | Problem |\n|--------------|---------|\n| Embedding similarity | Misses keyword-level matches |\n| Keyword overlap | Misses semantic similarity |\n| Topic matching | Too coarse-grained |\n| Structure matching | Ignores content |\n\nCombined scoring is more robust because weaknesses of one signal are compensated by strengths of another.\n\n### Algorithms Used\n\n#### 1. Cosine Similarity (Semantic Matching)\n```python\ndef cosine_similarity(a, b):\n    dot = np.dot(a, b)\n    return dot / (np.linalg.norm(a) * np.linalg.norm(b))\n```\n- Range: [-1, 1] \u2192 mapped to [0, 1]\n- Captures semantic meaning via embeddings\n- Weight: **40%** (highest because most robust)\n\n#### 2. Jaccard Similarity (Keyword Overlap)\n```python\ndef jaccard(set_a, set_b):\n    intersection = len(set_a & set_b)\n    union = len(set_a | set_b)\n    return intersection / union\n```\n- Range: [0, 1]\n- Captures exact keyword matches\n- Weight: **15%**\n\n#### 3. Topic Overlap (Long Word Matching)\n```python\ndef topic_overlap(query_words, pattern_words):\n    query_topics = {w for w in query_words if len(w) >= 5}\n    pattern_topics = {w for w in pattern_words if len(w) >= 5}\n    matches = query_topics & pattern_topics\n    return min(1.0, len(matches) * 0.5)\n```\n- Long words (\u22655 chars) are more likely to be meaningful topics\n- Weight: **20%**\n\n#### 4. Structure Similarity (Question Pattern Matching)\n```python\nPATTERNS = [r'^what is\\b', r'^how to\\b', r'^why\\b', ...]\n\ndef structure_similarity(query, pattern_trigger):\n    query_pattern = find_matching_pattern(query)\n    trigger_pattern = find_matching_pattern(pattern_trigger)\n    if query_pattern == trigger_pattern:\n        return 1.0  # Same question type\n    elif both_have_patterns:\n        return 0.3  # Different question types\n    return 0.0     # No structure match\n```\n- Matches question forms: \"what is X\" vs \"how to Y\"\n- Weight: **10%**\n\n#### 5. Orchestrator Keyword Boost\n```python\ndef keyword_boost(boost_keywords, pattern_trigger):\n    matches = sum(1 for kw in boost_keywords if kw in pattern_trigger)\n    return min(1.0, matches * 0.3)\n```\n- External system can boost specific keywords\n- Weight: **15%**\n\n### The Combined Formula\n\n```\nrelevance = 0.40 \u00d7 semantic + 0.15 \u00d7 keyword + 0.20 \u00d7 topic + 0.10 \u00d7 structure + 0.15 \u00d7 boost\n```\n\n### What the Code Demonstrates\n\n1. **PatternScorer** - Multi-factor scoring engine\n2. **score()** - Computes all factors and weighted combination\n3. **ScoredPattern** - Result with full breakdown\n4. **Demo** - Score patterns with detailed factor visualization\n\n### Mental Model\n\nThink of pattern scoring like evaluating job candidates:\n- \ud83d\udcca **Semantic** = Overall qualifications (big picture fit)\n- \ud83d\udd11 **Keywords** = Required skills checklist\n- \ud83c\udfaf **Topics** = Domain expertise\n- \ud83d\udcdd **Structure** = Communication style match\n- \u2b50 **Boost** = Referral bonus"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScoredPattern:\n",
    "    \"\"\"Pattern with detailed relevance score breakdown.\"\"\"\n",
    "    pattern_id: str\n",
    "    trigger: str\n",
    "    strategy: str\n",
    "    relevance: float\n",
    "    \n",
    "    # Score breakdown\n",
    "    semantic_similarity: float\n",
    "    keyword_overlap: float\n",
    "    topic_overlap: float\n",
    "    structure_similarity: float\n",
    "\n",
    "class PatternScorer:\n",
    "    \"\"\"Multi-factor pattern relevance scoring.\"\"\"\n",
    "    \n",
    "    QUESTION_PATTERNS = [\n",
    "        r'^what is\\b', r'^what are\\b', r'^how to\\b', r'^how do\\b',\n",
    "        r'^why\\b', r'^explain\\b', r'^describe\\b', r'^tell me about\\b',\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_semantic: float = 0.40,\n",
    "        weight_keyword: float = 0.15,\n",
    "        weight_topic: float = 0.20,\n",
    "        weight_structure: float = 0.10,\n",
    "        weight_boost: float = 0.15,\n",
    "    ):\n",
    "        self.weight_semantic = weight_semantic\n",
    "        self.weight_keyword = weight_keyword\n",
    "        self.weight_topic = weight_topic\n",
    "        self.weight_structure = weight_structure\n",
    "        self.weight_boost = weight_boost\n",
    "        self.stopwords = PatternGraph.STOPWORDS\n",
    "    \n",
    "    def score(\n",
    "        self,\n",
    "        query: str,\n",
    "        pattern: dict,\n",
    "        query_embedding: np.ndarray | None = None,\n",
    "        pattern_embedding: np.ndarray | None = None,\n",
    "        boost_keywords: list[str] | None = None,\n",
    "    ) -> ScoredPattern:\n",
    "        \"\"\"Score a pattern for relevance to query.\"\"\"\n",
    "        trigger = pattern.get(\"trigger\", \"\")\n",
    "        strategy = pattern.get(\"strategy\", \"\")\n",
    "        \n",
    "        # 1. Semantic similarity (embedding-based)\n",
    "        if query_embedding is not None and pattern_embedding is not None:\n",
    "            dot = np.dot(query_embedding, pattern_embedding)\n",
    "            norm_q = np.linalg.norm(query_embedding)\n",
    "            norm_p = np.linalg.norm(pattern_embedding)\n",
    "            semantic = float(np.clip((dot / (norm_q * norm_p + 1e-8) + 1) / 2, 0, 1))\n",
    "        else:\n",
    "            semantic = 0.5\n",
    "        \n",
    "        # 2. Keyword overlap (Jaccard similarity)\n",
    "        query_kw = self._extract_keywords(query)\n",
    "        trigger_kw = self._extract_keywords(trigger)\n",
    "        keyword_overlap = self._jaccard(query_kw, trigger_kw)\n",
    "        \n",
    "        # 3. Topic overlap (5+ char words for better signal)\n",
    "        query_topics = {w for w in query_kw if len(w) >= 5}\n",
    "        trigger_topics = {w for w in trigger_kw if len(w) >= 5}\n",
    "        topic_intersection = query_topics & trigger_topics\n",
    "        topic_overlap = min(1.0, len(topic_intersection) * 0.5) if topic_intersection else 0.0\n",
    "        \n",
    "        # 4. Structure similarity (question pattern matching)\n",
    "        structure = self._structure_similarity(query.lower(), trigger.lower())\n",
    "        \n",
    "        # 5. Keyword boost from orchestrator\n",
    "        boost = 0.0\n",
    "        if boost_keywords:\n",
    "            matches = sum(1 for kw in boost_keywords if kw.lower() in trigger.lower())\n",
    "            boost = min(1.0, matches * 0.3)\n",
    "        \n",
    "        # Weighted combination\n",
    "        relevance = (\n",
    "            self.weight_semantic * semantic +\n",
    "            self.weight_keyword * keyword_overlap +\n",
    "            self.weight_topic * topic_overlap +\n",
    "            self.weight_structure * structure +\n",
    "            self.weight_boost * boost\n",
    "        )\n",
    "        \n",
    "        return ScoredPattern(\n",
    "            pattern_id=pattern.get(\"id\", \"\"),\n",
    "            trigger=trigger,\n",
    "            strategy=strategy,\n",
    "            relevance=relevance,\n",
    "            semantic_similarity=semantic,\n",
    "            keyword_overlap=keyword_overlap,\n",
    "            topic_overlap=topic_overlap,\n",
    "            structure_similarity=structure,\n",
    "        )\n",
    "    \n",
    "    def _extract_keywords(self, text: str) -> set[str]:\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "        return {w for w in words if w not in self.stopwords}\n",
    "    \n",
    "    def _jaccard(self, set_a: set, set_b: set) -> float:\n",
    "        if not set_a or not set_b:\n",
    "            return 0.0\n",
    "        intersection = len(set_a & set_b)\n",
    "        union = len(set_a | set_b)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def _structure_similarity(self, query: str, trigger: str) -> float:\n",
    "        query_pattern = None\n",
    "        trigger_pattern = None\n",
    "        \n",
    "        for pattern in self.QUESTION_PATTERNS:\n",
    "            if re.search(pattern, query):\n",
    "                query_pattern = pattern\n",
    "            if re.search(pattern, trigger):\n",
    "                trigger_pattern = pattern\n",
    "        \n",
    "        if query_pattern and trigger_pattern:\n",
    "            return 1.0 if query_pattern == trigger_pattern else 0.3\n",
    "        return 0.0\n",
    "\n",
    "pattern_scorer = PatternScorer()\n",
    "print(\"PatternScorer ready with multi-factor scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Score patterns with detailed breakdown\n",
    "\n",
    "query = \"How do I deploy my application to production?\"\n",
    "\n",
    "# Generate query embedding\n",
    "np.random.seed(hash(query) % 2**32)\n",
    "query_emb = np.random.randn(384).astype(np.float32)\n",
    "query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for p in patterns:\n",
    "    # Generate pattern embedding\n",
    "    np.random.seed(hash(p[\"trigger\"]) % 2**32)\n",
    "    pattern_emb = np.random.randn(384).astype(np.float32)\n",
    "    pattern_emb = pattern_emb / np.linalg.norm(pattern_emb)\n",
    "    \n",
    "    scored = pattern_scorer.score(\n",
    "        query=query,\n",
    "        pattern=p,\n",
    "        query_embedding=query_emb,\n",
    "        pattern_embedding=pattern_emb,\n",
    "        boost_keywords=[\"deploy\", \"production\"],\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPattern: {scored.trigger[:50]}...\")\n",
    "    print(f\"  TOTAL RELEVANCE: {scored.relevance:.3f}\")\n",
    "    print(f\"  Breakdown:\")\n",
    "    print(f\"    Semantic:  {scored.semantic_similarity:.3f} (x{pattern_scorer.weight_semantic})\")\n",
    "    print(f\"    Keyword:   {scored.keyword_overlap:.3f} (x{pattern_scorer.weight_keyword})\")\n",
    "    print(f\"    Topic:     {scored.topic_overlap:.3f} (x{pattern_scorer.weight_topic})\")\n",
    "    print(f\"    Structure: {scored.structure_similarity:.3f} (x{pattern_scorer.weight_structure})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 9: Mode Selection\n\n### The Concept\n\n**Mode Selection** determines HOW the agent should respond based on available context. Different modes have different costs, latencies, and capabilities.\n\n### Why This Matters for AI Agents\n\nNot every query needs the same response strategy:\n\n| Query Type | Inefficient | Efficient |\n|------------|-------------|-----------|\n| \"What is X?\" (known pattern) | Full LLM reasoning | Direct pattern response |\n| \"Help me debug\" (needs context) | Generic answer | Agent with memory retrieval |\n| \"Analyze and summarize all\" | Single LLM call | Multi-step workflow |\n| \"Quick fact check\" | Complex pipeline | Fast lookup + response |\n\n### The Four Response Modes\n\n| Mode | Use Case | Cost | Latency | Description |\n|------|----------|------|---------|-------------|\n| **FAST** | Simple queries with context | Low | <500ms | Single LLM call with retrieved memories |\n| **AGENT** | Exploration needed | Medium | 1-10s | Multi-step ReAct agent with tool use |\n| **PATTERN_DIRECT** | High-confidence pattern | Very Low | <100ms | Return pattern strategy without LLM |\n| **WORKFLOW** | Complex multi-step tasks | High | 10s+ | DAG-based workflow execution |\n\n### The Decision Algorithm\n\n```python\ndef select_mode(query, memories, patterns):\n    # Priority 1: Complex tasks \u2192 WORKFLOW\n    if contains_complex_keywords(query):\n        return WORKFLOW\n    \n    # Priority 2: High-confidence pattern \u2192 PATTERN_DIRECT\n    if patterns and best_pattern.relevance >= 0.7:\n        return PATTERN_DIRECT\n    \n    # Priority 3: Have memories \u2192 FAST\n    if len(memories) >= 1:\n        return FAST\n    \n    # Default: Need exploration \u2192 AGENT\n    return AGENT\n```\n\n### Decision Factors\n\n#### Complex Task Detection\n```python\nCOMPLEX_KEYWORDS = [\n    \"analyze\", \"workflow\", \"process\", \"multiple\", \"steps\",\n    \"first then\", \"summarize all\", \"compare all\", \"review all\"\n]\n\ndef is_complex(query):\n    return any(kw in query.lower() for kw in COMPLEX_KEYWORDS)\n```\n\n#### Pattern Confidence Threshold\n- Threshold: 0.7 (configurable)\n- Above threshold: Pattern is reliable enough to use directly\n- Below threshold: Pattern is suggestive but needs LLM verification\n\n#### Memory Sufficiency\n- Minimum: 1 memory (configurable)\n- Presence of memories means we have context to answer\n- No memories = need to explore (AGENT mode)\n\n### What the Code Demonstrates\n\n1. **ResponseMode** enum - The four available modes\n2. **ModeDecision** - Result with mode, reason, and confidence\n3. **ModeSelector** - Decision logic implementation\n4. **Demo** - Different contexts trigger different modes\n\n### Mental Model\n\nThink of mode selection like choosing transportation:\n- \ud83d\ude80 **PATTERN_DIRECT** = Take the shortcut (you know the way)\n- \ud83d\ude97 **FAST** = Drive direct (have a map)\n- \ud83d\uddfa\ufe0f **AGENT** = Need GPS navigation (exploring)\n- \ud83d\ude82 **WORKFLOW** = Multi-leg journey (complex trip)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseMode(str, Enum):\n",
    "    \"\"\"Available response modes.\"\"\"\n",
    "    FAST = \"fast\"               # Single LLM call with context\n",
    "    AGENT = \"agent\"             # Multi-step ReAct agent\n",
    "    PATTERN_DIRECT = \"pattern\"  # Direct pattern response (no LLM)\n",
    "    WORKFLOW = \"workflow\"       # DAG-based workflow execution\n",
    "\n",
    "@dataclass\n",
    "class ModeDecision:\n",
    "    \"\"\"Result of mode selection.\"\"\"\n",
    "    mode: ResponseMode\n",
    "    reason: str\n",
    "    confidence: float = 1.0\n",
    "    \n",
    "    # Context flags\n",
    "    has_memories: bool = False\n",
    "    has_patterns: bool = False\n",
    "    \n",
    "    # For pattern_direct mode\n",
    "    direct_response: str | None = None\n",
    "\n",
    "class ModeSelector:\n",
    "    \"\"\"Selects optimal response mode based on available context.\"\"\"\n",
    "    \n",
    "    COMPLEX_TASK_KEYWORDS = [\n",
    "        \"analyze\", \"workflow\", \"process\", \"multiple\", \"steps\",\n",
    "        \"first then\", \"summarize all\", \"compare all\", \"review all\",\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        min_memories_for_fast: int = 1,\n",
    "        pattern_direct_threshold: float = 0.7,\n",
    "    ):\n",
    "        self.min_memories_for_fast = min_memories_for_fast\n",
    "        self.pattern_direct_threshold = pattern_direct_threshold\n",
    "    \n",
    "    def select(\n",
    "        self,\n",
    "        query: str,\n",
    "        memories: list | None = None,\n",
    "        patterns: list[ScoredPattern] | None = None,\n",
    "        conversation_turns: int = 0,\n",
    "    ) -> ModeDecision:\n",
    "        \"\"\"Select optimal response mode.\"\"\"\n",
    "        memories = memories or []\n",
    "        patterns = patterns or []\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        has_memories = len(memories) >= self.min_memories_for_fast\n",
    "        has_patterns = len(patterns) > 0\n",
    "        \n",
    "        # 1. Check for complex multi-step task\n",
    "        is_complex = any(kw in query_lower for kw in self.COMPLEX_TASK_KEYWORDS)\n",
    "        if is_complex:\n",
    "            return ModeDecision(\n",
    "                mode=ResponseMode.WORKFLOW,\n",
    "                reason=\"complex_task_detected\",\n",
    "                confidence=0.8,\n",
    "                has_memories=has_memories,\n",
    "                has_patterns=has_patterns,\n",
    "            )\n",
    "        \n",
    "        # 2. Check for high-confidence pattern\n",
    "        if patterns:\n",
    "            best = max(patterns, key=lambda p: p.relevance)\n",
    "            if best.relevance >= self.pattern_direct_threshold:\n",
    "                return ModeDecision(\n",
    "                    mode=ResponseMode.PATTERN_DIRECT,\n",
    "                    reason=\"high_relevance_pattern\",\n",
    "                    confidence=best.relevance,\n",
    "                    has_memories=has_memories,\n",
    "                    has_patterns=True,\n",
    "                    direct_response=best.strategy,\n",
    "                )\n",
    "        \n",
    "        # 3. Check for sufficient memories\n",
    "        if has_memories:\n",
    "            return ModeDecision(\n",
    "                mode=ResponseMode.FAST,\n",
    "                reason=\"memories_found\",\n",
    "                confidence=0.9,\n",
    "                has_memories=True,\n",
    "                has_patterns=has_patterns,\n",
    "            )\n",
    "        \n",
    "        # 4. Default to agent mode (needs exploration)\n",
    "        return ModeDecision(\n",
    "            mode=ResponseMode.AGENT,\n",
    "            reason=\"insufficient_context\",\n",
    "            confidence=0.6,\n",
    "            has_memories=has_memories,\n",
    "            has_patterns=has_patterns,\n",
    "        )\n",
    "\n",
    "mode_selector = ModeSelector()\n",
    "print(\"ModeSelector ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Mode selection with different contexts\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Simple query with memories\",\n",
    "        \"query\": \"What is Python?\",\n",
    "        \"memories\": sample_memories[:2],\n",
    "        \"patterns\": [],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Query with high-confidence pattern\",\n",
    "        \"query\": \"How to deploy?\",\n",
    "        \"memories\": [],\n",
    "        \"patterns\": [ScoredPattern(\"p1\", \"How to deploy?\", \"Use docker compose up -d\", 0.85, 0.8, 0.7, 0.6, 1.0)],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Complex multi-step task\",\n",
    "        \"query\": \"Analyze all the data and then summarize the findings in multiple steps\",\n",
    "        \"memories\": sample_memories,\n",
    "        \"patterns\": [],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"No context available\",\n",
    "        \"query\": \"Random question about something obscure\",\n",
    "        \"memories\": [],\n",
    "        \"patterns\": [],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Mode Selection Demo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for case in test_cases:\n",
    "    decision = mode_selector.select(\n",
    "        query=case[\"query\"],\n",
    "        memories=case[\"memories\"],\n",
    "        patterns=case[\"patterns\"],\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{case['name']}:\")\n",
    "    print(f\"  Query: {case['query'][:50]}...\")\n",
    "    print(f\"  Context: {len(case['memories'])} memories, {len(case['patterns'])} patterns\")\n",
    "    print(f\"  -> Mode: {decision.mode.value.upper()}\")\n",
    "    print(f\"     Reason: {decision.reason}\")\n",
    "    print(f\"     Confidence: {decision.confidence:.2f}\")\n",
    "    if decision.direct_response:\n",
    "        print(f\"     Direct response: {decision.direct_response[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 9B: LLM-Driven Orchestration\n\n### The Concept\n\nIn production HGM, the **Orchestrator** uses an LLM to make intelligent decisions that go beyond rule-based logic. The LLM provides:\n\n1. **Contextual Temperature Adjustment** - Understanding *why* a memory is relevant, not just similarity scores\n2. **Semantic Promotion Decisions** - Promoting memories based on reasoning, not just thresholds\n3. **Dynamic Keyword Extraction** - Identifying important concepts the pattern graph might miss\n4. **Mode Selection Reasoning** - Explaining why a particular response mode was chosen\n\n### Why LLM-Driven Decisions Matter\n\nRule-based systems have limitations:\n\n| Rule-Based | LLM-Driven |\n|------------|------------|\n| Fixed thresholds (temp > 0.7 \u2192 promote) | Contextual reasoning (\"this is relevant because...\") |\n| Keyword matching only | Semantic understanding of concepts |\n| Binary decisions | Nuanced confidence with explanations |\n| Can't handle novel situations | Generalizes to new contexts |\n\n### The Orchestrator Pattern\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           ORCHESTRATOR FLOW                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                              \u2502\n\u2502   USER QUERY                                                                 \u2502\n\u2502       \u2502                                                                      \u2502\n\u2502       \u25bc                                                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n\u2502   \u2502            INITIAL RETRIEVAL                 \u2502                           \u2502\n\u2502   \u2502  \u2022 Pattern graph lookup                      \u2502                           \u2502\n\u2502   \u2502  \u2022 Three-tier memory search                  \u2502                           \u2502\n\u2502   \u2502  \u2022 Temperature scoring (rule-based)          \u2502                           \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n\u2502       \u2502                                                                      \u2502\n\u2502       \u25bc                                                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n\u2502   \u2502         LLM ORCHESTRATOR ANALYSIS            \u2502            \u2502\n\u2502   \u2502  \u2022 Evaluate memory relevance (0-1)           \u2502                           \u2502\n\u2502   \u2502  \u2022 Suggest temperature adjustments           \u2502                           \u2502\n\u2502   \u2502  \u2022 Extract additional keywords               \u2502                           \u2502\n\u2502   \u2502  \u2022 Recommend response mode                   \u2502                           \u2502\n\u2502   \u2502  \u2022 Provide reasoning                         \u2502                           \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n\u2502       \u2502                                                                      \u2502\n\u2502       \u25bc                                                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n\u2502   \u2502         ADJUSTED DECISIONS                   \u2502                           \u2502\n\u2502   \u2502  \u2022 Apply LLM temperature adjustments         \u2502                           \u2502\n\u2502   \u2502  \u2022 Promote/demote based on LLM reasoning     \u2502                           \u2502\n\u2502   \u2502  \u2022 Use LLM-selected mode                     \u2502                           \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n\u2502       \u2502                                                                      \u2502\n\u2502       \u25bc                                                                      \u2502\n\u2502   RESPONSE GENERATION                                                        \u2502\n\u2502                                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### What the Code Demonstrates\n\n1. **OrchestratorLLM** - Interface for LLM-driven decisions (supports OpenAI, Anthropic, or mock)\n2. **analyze_memories()** - LLM evaluates memory relevance with reasoning\n3. **suggest_promotions()** - LLM recommends tier movements\n4. **Temperature adjustment** - Combining rule-based scores with LLM judgment\n5. **Demo** - See how LLM reasoning improves decisions\n\n### Configuration\n\nSet environment variables for real LLM integration:\n```bash\nexport OPENAI_API_KEY=\"sk-...\"        # For OpenAI\nexport ANTHROPIC_API_KEY=\"sk-ant-...\" # For Anthropic\n```\n\nOr run without API keys to use the mock LLM (demonstrates the pattern without costs)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# LLM Orchestrator for intelligent decision-making\nimport os\nimport json as json_module\nfrom abc import ABC, abstractmethod\n\n# =============================================================================\n# LLM INTERFACE\n# =============================================================================\n\nclass BaseLLM(ABC):\n    \"\"\"Abstract base class for LLM providers.\"\"\"\n    \n    @abstractmethod\n    def complete(self, prompt: str, system: str = \"\") -> str:\n        \"\"\"Generate a completion from the LLM.\"\"\"\n        pass\n\nclass MockLLM(BaseLLM):\n    \"\"\"\n    Mock LLM for demonstration without API keys.\n    Uses heuristics to simulate LLM-like reasoning.\n    \"\"\"\n    \n    def complete(self, prompt: str, system: str = \"\") -> str:\n        # Parse the prompt to understand what's being asked\n        prompt_lower = prompt.lower()\n        \n        # Simulate memory relevance analysis\n        if \"evaluate the relevance\" in prompt_lower or \"analyze these memories\" in prompt_lower:\n            return self._mock_memory_analysis(prompt)\n        \n        # Simulate promotion suggestions\n        if \"suggest which memories should be promoted\" in prompt_lower:\n            return self._mock_promotion_suggestion(prompt)\n        \n        # Simulate mode selection\n        if \"select the best response mode\" in prompt_lower:\n            return self._mock_mode_selection(prompt)\n        \n        # Default response\n        return json_module.dumps({\"response\": \"Mock LLM response\", \"confidence\": 0.5})\n    \n    def _mock_memory_analysis(self, prompt: str) -> str:\n        \"\"\"Simulate memory relevance analysis.\"\"\"\n        # Extract query topic from prompt (simplified heuristic)\n        analyses = []\n        \n        # Look for memory content patterns in the prompt\n        if \"python\" in prompt.lower():\n            analyses.append({\n                \"memory_index\": 0,\n                \"relevance\": 0.85,\n                \"reasoning\": \"Directly discusses Python programming concepts\",\n                \"suggested_temperature_adjustment\": 0.15\n            })\n        if \"deploy\" in prompt.lower():\n            analyses.append({\n                \"memory_index\": 2,\n                \"relevance\": 0.90,\n                \"reasoning\": \"Contains deployment instructions matching the query intent\",\n                \"suggested_temperature_adjustment\": 0.20\n            })\n        if \"api\" in prompt.lower() or \"auth\" in prompt.lower():\n            analyses.append({\n                \"memory_index\": 1,\n                \"relevance\": 0.75,\n                \"reasoning\": \"Related to API and authentication topics\",\n                \"suggested_temperature_adjustment\": 0.10\n            })\n        \n        # Default analysis if no specific matches\n        if not analyses:\n            analyses = [{\n                \"memory_index\": 0,\n                \"relevance\": 0.50,\n                \"reasoning\": \"Moderate topical relevance\",\n                \"suggested_temperature_adjustment\": 0.0\n            }]\n        \n        return json_module.dumps({\"analyses\": analyses})\n    \n    def _mock_promotion_suggestion(self, prompt: str) -> str:\n        \"\"\"Simulate promotion suggestions.\"\"\"\n        suggestions = []\n        \n        if \"deploy\" in prompt.lower() and \"cold\" in prompt.lower():\n            suggestions.append({\n                \"memory_id\": \"deployment_memory\",\n                \"action\": \"promote_to_hot\",\n                \"reasoning\": \"Deployment knowledge is critical for current query\"\n            })\n        \n        return json_module.dumps({\"suggestions\": suggestions})\n    \n    def _mock_mode_selection(self, prompt: str) -> str:\n        \"\"\"Simulate mode selection.\"\"\"\n        if \"analyze\" in prompt.lower() or \"multiple\" in prompt.lower():\n            return json_module.dumps({\n                \"mode\": \"WORKFLOW\",\n                \"confidence\": 0.85,\n                \"reasoning\": \"Query requires multi-step analysis\"\n            })\n        elif \"pattern\" in prompt.lower() and \"high\" in prompt.lower():\n            return json_module.dumps({\n                \"mode\": \"PATTERN_DIRECT\",\n                \"confidence\": 0.90,\n                \"reasoning\": \"High-confidence pattern match available\"\n            })\n        else:\n            return json_module.dumps({\n                \"mode\": \"FAST\",\n                \"confidence\": 0.75,\n                \"reasoning\": \"Sufficient context available for direct response\"\n            })\n\nclass OpenAILLM(BaseLLM):\n    \"\"\"OpenAI API integration.\"\"\"\n    \n    def __init__(self, model: str = \"gpt-4o-mini\"):\n        self.model = model\n        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"OPENAI_API_KEY not set\")\n    \n    def complete(self, prompt: str, system: str = \"\") -> str:\n        try:\n            import openai\n            client = openai.OpenAI(api_key=self.api_key)\n            \n            messages = []\n            if system:\n                messages.append({\"role\": \"system\", \"content\": system})\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            \n            response = client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                temperature=0.1,  # Low temperature for consistent decisions\n                response_format={\"type\": \"json_object\"}\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f\"OpenAI API error: {e}\")\n            return json_module.dumps({\"error\": str(e)})\n\nclass AnthropicLLM(BaseLLM):\n    \"\"\"Anthropic API integration.\"\"\"\n    \n    def __init__(self, model: str = \"claude-3-5-haiku-20241022\"):\n        self.model = model\n        self.api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"ANTHROPIC_API_KEY not set\")\n    \n    def complete(self, prompt: str, system: str = \"\") -> str:\n        try:\n            import anthropic\n            client = anthropic.Anthropic(api_key=self.api_key)\n            \n            response = client.messages.create(\n                model=self.model,\n                max_tokens=1024,\n                system=system or \"You are a memory orchestration assistant. Respond in JSON format.\",\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            return response.content[0].text\n        except Exception as e:\n            print(f\"Anthropic API error: {e}\")\n            return json_module.dumps({\"error\": str(e)})\n\ndef get_llm(provider: str = \"auto\") -> BaseLLM:\n    \"\"\"\n    Get an LLM instance based on available API keys.\n    \n    Args:\n        provider: \"openai\", \"anthropic\", \"mock\", or \"auto\" (try real APIs first)\n    \"\"\"\n    if provider == \"mock\":\n        return MockLLM()\n    \n    if provider == \"auto\":\n        # Try OpenAI first\n        if os.environ.get(\"OPENAI_API_KEY\"):\n            try:\n                return OpenAILLM()\n            except Exception:\n                pass\n        \n        # Try Anthropic\n        if os.environ.get(\"ANTHROPIC_API_KEY\"):\n            try:\n                return AnthropicLLM()\n            except Exception:\n                pass\n        \n        # Fall back to mock\n        print(\"No API keys found. Using MockLLM for demonstration.\")\n        return MockLLM()\n    \n    if provider == \"openai\":\n        return OpenAILLM()\n    elif provider == \"anthropic\":\n        return AnthropicLLM()\n    else:\n        return MockLLM()\n\nprint(\"LLM providers defined: MockLLM, OpenAILLM, AnthropicLLM\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# LLM ORCHESTRATOR\n# =============================================================================\n\n@dataclass\nclass MemoryAnalysis:\n    \"\"\"LLM analysis of a memory's relevance.\"\"\"\n    memory_id: str\n    original_temperature: float\n    llm_relevance: float\n    adjusted_temperature: float\n    reasoning: str\n    should_promote: bool\n    should_demote: bool\n\n@dataclass \nclass OrchestratorDecision:\n    \"\"\"Complete orchestrator decision with LLM reasoning.\"\"\"\n    mode: ResponseMode\n    mode_reasoning: str\n    memory_analyses: list[MemoryAnalysis]\n    extracted_keywords: list[str]\n    confidence: float\n    \nclass LLMOrchestrator:\n    \"\"\"\n    Orchestrator that uses LLM reasoning for intelligent decisions.\n    \n    This mirrors the HGM Mind's orchestrator which combines rule-based\n    retrieval with LLM-driven analysis for better decisions.\n    \"\"\"\n    \n    SYSTEM_PROMPT = \"\"\"You are a memory orchestration system for an AI agent.\nYour job is to analyze memories and make intelligent decisions about:\n1. How relevant each memory is to the current query\n2. Whether memories should be promoted (moved to faster storage) or demoted\n3. What keywords/concepts are important in the query\n4. What response mode the agent should use\n\nAlways respond in valid JSON format.\"\"\"\n\n    MEMORY_ANALYSIS_PROMPT = \"\"\"Analyze the relevance of these memories to the user's query.\n\nQuery: {query}\n\nMemories to analyze:\n{memories}\n\nFor each memory, provide:\n1. relevance: A score from 0.0 to 1.0\n2. reasoning: Why this memory is or isn't relevant (1-2 sentences)\n3. suggested_temperature_adjustment: How much to adjust the temperature (-0.3 to +0.3)\n\nRespond in JSON format:\n{{\n    \"analyses\": [\n        {{\n            \"memory_index\": 0,\n            \"relevance\": 0.85,\n            \"reasoning\": \"Directly addresses the deployment question\",\n            \"suggested_temperature_adjustment\": 0.15\n        }}\n    ],\n    \"extracted_keywords\": [\"deploy\", \"docker\", \"production\"],\n    \"overall_assessment\": \"The memories contain relevant deployment information\"\n}}\"\"\"\n\n    MODE_SELECTION_PROMPT = \"\"\"Select the best response mode for this query.\n\nQuery: {query}\n\nAvailable context:\n- Memories found: {memory_count}\n- Best pattern match: {best_pattern} (relevance: {pattern_relevance})\n- Query complexity indicators: {complexity_indicators}\n\nAvailable modes:\n- FAST: Single LLM call with retrieved context (best for simple queries with good context)\n- AGENT: Multi-step exploration (best when context is insufficient)\n- PATTERN_DIRECT: Use pattern strategy directly (best for high-confidence pattern matches)\n- WORKFLOW: Multi-step DAG execution (best for complex multi-part tasks)\n\nRespond in JSON:\n{{\n    \"mode\": \"FAST\",\n    \"confidence\": 0.85,\n    \"reasoning\": \"Sufficient context available and query is straightforward\"\n}}\"\"\"\n    \n    def __init__(self, llm: BaseLLM | None = None, temperature_scorer: TemperatureScorer | None = None):\n        self.llm = llm or get_llm(\"auto\")\n        self.scorer = temperature_scorer or TemperatureScorer()\n        self._is_mock = isinstance(self.llm, MockLLM)\n    \n    def analyze_memories(\n        self,\n        query: str,\n        memories: list[Memory],\n        focus_embedding: np.ndarray | None = None,\n        focus_entities: set[str] | None = None,\n    ) -> list[MemoryAnalysis]:\n        \"\"\"\n        Use LLM to analyze memory relevance and suggest temperature adjustments.\n        \n        This combines rule-based temperature scoring with LLM reasoning.\n        \"\"\"\n        analyses = []\n        \n        # First, compute rule-based temperatures\n        for mem in memories:\n            rule_temp = self.scorer.compute(\n                memory=mem,\n                focus_embedding=focus_embedding,\n                focus_entities=focus_entities,\n            )\n            mem.temperature = rule_temp\n        \n        # Format memories for LLM\n        memory_descriptions = []\n        for i, mem in enumerate(memories):\n            memory_descriptions.append(\n                f\"{i}. [{mem.memory_type.value}] (temp: {mem.temperature:.2f}) {mem.content[:100]}...\"\n            )\n        \n        # Get LLM analysis\n        prompt = self.MEMORY_ANALYSIS_PROMPT.format(\n            query=query,\n            memories=\"\\n\".join(memory_descriptions)\n        )\n        \n        try:\n            response = self.llm.complete(prompt, self.SYSTEM_PROMPT)\n            result = json_module.loads(response)\n            \n            for analysis in result.get(\"analyses\", []):\n                idx = analysis.get(\"memory_index\", 0)\n                if idx < len(memories):\n                    mem = memories[idx]\n                    llm_relevance = analysis.get(\"relevance\", 0.5)\n                    adjustment = analysis.get(\"suggested_temperature_adjustment\", 0.0)\n                    \n                    # Combine rule-based and LLM temperatures (weighted average)\n                    # LLM gets 40% weight, rule-based gets 60%\n                    adjusted_temp = 0.6 * mem.temperature + 0.4 * llm_relevance\n                    adjusted_temp = min(1.0, max(0.0, adjusted_temp + adjustment))\n                    \n                    analyses.append(MemoryAnalysis(\n                        memory_id=mem.id,\n                        original_temperature=mem.temperature,\n                        llm_relevance=llm_relevance,\n                        adjusted_temperature=adjusted_temp,\n                        reasoning=analysis.get(\"reasoning\", \"\"),\n                        should_promote=adjusted_temp >= 0.70 and mem.temperature < 0.70,\n                        should_demote=adjusted_temp < 0.50 and mem.temperature >= 0.50,\n                    ))\n        except Exception as e:\n            print(f\"LLM analysis error: {e}\")\n            # Fall back to rule-based only\n            for mem in memories:\n                analyses.append(MemoryAnalysis(\n                    memory_id=mem.id,\n                    original_temperature=mem.temperature,\n                    llm_relevance=mem.temperature,\n                    adjusted_temperature=mem.temperature,\n                    reasoning=\"Rule-based scoring (LLM unavailable)\",\n                    should_promote=mem.temperature >= 0.70,\n                    should_demote=mem.temperature < 0.50,\n                ))\n        \n        return analyses\n    \n    def select_mode(\n        self,\n        query: str,\n        memories: list[Memory],\n        patterns: list[ScoredPattern],\n    ) -> tuple[ResponseMode, str, float]:\n        \"\"\"\n        Use LLM to select the optimal response mode with reasoning.\n        \"\"\"\n        best_pattern = patterns[0] if patterns else None\n        \n        # Detect complexity indicators\n        complexity_indicators = []\n        query_lower = query.lower()\n        if any(kw in query_lower for kw in [\"analyze\", \"compare\", \"summarize all\"]):\n            complexity_indicators.append(\"analysis_required\")\n        if any(kw in query_lower for kw in [\"step by step\", \"workflow\", \"process\"]):\n            complexity_indicators.append(\"multi_step\")\n        if len(query.split()) < 5:\n            complexity_indicators.append(\"short_query\")\n        \n        prompt = self.MODE_SELECTION_PROMPT.format(\n            query=query,\n            memory_count=len(memories),\n            best_pattern=best_pattern.trigger[:50] if best_pattern else \"None\",\n            pattern_relevance=f\"{best_pattern.relevance:.2f}\" if best_pattern else \"N/A\",\n            complexity_indicators=\", \".join(complexity_indicators) or \"none\"\n        )\n        \n        try:\n            response = self.llm.complete(prompt, self.SYSTEM_PROMPT)\n            result = json_module.loads(response)\n            \n            mode_str = result.get(\"mode\", \"FAST\").upper()\n            mode_map = {\n                \"FAST\": ResponseMode.FAST,\n                \"AGENT\": ResponseMode.AGENT,\n                \"PATTERN_DIRECT\": ResponseMode.PATTERN_DIRECT,\n                \"PATTERN\": ResponseMode.PATTERN_DIRECT,\n                \"WORKFLOW\": ResponseMode.WORKFLOW,\n            }\n            mode = mode_map.get(mode_str, ResponseMode.FAST)\n            reasoning = result.get(\"reasoning\", \"\")\n            confidence = result.get(\"confidence\", 0.5)\n            \n            return mode, reasoning, confidence\n            \n        except Exception as e:\n            print(f\"LLM mode selection error: {e}\")\n            # Fall back to rule-based\n            if best_pattern and best_pattern.relevance >= 0.7:\n                return ResponseMode.PATTERN_DIRECT, \"High-confidence pattern (fallback)\", 0.7\n            elif memories:\n                return ResponseMode.FAST, \"Memories available (fallback)\", 0.6\n            else:\n                return ResponseMode.AGENT, \"Insufficient context (fallback)\", 0.5\n    \n    def orchestrate(\n        self,\n        query: str,\n        memories: list[Memory],\n        patterns: list[ScoredPattern],\n        focus_embedding: np.ndarray | None = None,\n        focus_entities: set[str] | None = None,\n    ) -> OrchestratorDecision:\n        \"\"\"\n        Full orchestration: analyze memories, select mode, return decision.\n        \"\"\"\n        # Analyze memories with LLM\n        analyses = self.analyze_memories(query, memories, focus_embedding, focus_entities)\n        \n        # Select mode with LLM\n        mode, mode_reasoning, confidence = self.select_mode(query, memories, patterns)\n        \n        # Extract keywords from analysis (would come from LLM in full implementation)\n        keywords = list(focus_entities) if focus_entities else []\n        \n        return OrchestratorDecision(\n            mode=mode,\n            mode_reasoning=mode_reasoning,\n            memory_analyses=analyses,\n            extracted_keywords=keywords,\n            confidence=confidence,\n        )\n\n# Create orchestrator (will auto-detect available LLM)\norchestrator = LLMOrchestrator()\nprint(f\"LLMOrchestrator initialized (using {'MockLLM' if orchestrator._is_mock else 'Real LLM'})\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# DEMO: LLM-DRIVEN ORCHESTRATION\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"LLM-DRIVEN ORCHESTRATION DEMO\")\nprint(\"=\" * 70)\n\n# Test query\ntest_query = \"How do I deploy my Python application to production?\"\n\n# Create focus context\nnp.random.seed(hash(test_query) % 2**32)\nquery_embedding = np.random.randn(384).astype(np.float32)\nquery_embedding = query_embedding / np.linalg.norm(query_embedding)\nquery_entities = {\"deploy\", \"python\", \"production\", \"application\"}\n\nprint(f\"\\nQuery: {test_query}\")\nprint(f\"Focus entities: {query_entities}\")\n\n# Get orchestrator decision\ndecision = orchestrator.orchestrate(\n    query=test_query,\n    memories=sample_memories,\n    patterns=[],  # Would normally come from pattern graph\n    focus_embedding=query_embedding,\n    focus_entities=query_entities,\n)\n\nprint(\"\\n\" + \"-\" * 70)\nprint(\"MEMORY ANALYSES (LLM-Enhanced)\")\nprint(\"-\" * 70)\n\nfor analysis in decision.memory_analyses:\n    # Find the memory\n    mem = next((m for m in sample_memories if m.id == analysis.memory_id), None)\n    if mem:\n        print(f\"\\n[{mem.memory_type.value.upper()}] {mem.content[:50]}...\")\n        print(f\"  Original temperature: {analysis.original_temperature:.3f}\")\n        print(f\"  LLM relevance score:  {analysis.llm_relevance:.3f}\")\n        print(f\"  Adjusted temperature: {analysis.adjusted_temperature:.3f}\")\n        print(f\"  Reasoning: {analysis.reasoning}\")\n        \n        if analysis.should_promote:\n            print(f\"  \u2192 PROMOTE to hot tier\")\n        elif analysis.should_demote:\n            print(f\"  \u2192 DEMOTE from hot tier\")\n\nprint(\"\\n\" + \"-\" * 70)\nprint(\"MODE SELECTION (LLM-Enhanced)\")\nprint(\"-\" * 70)\nprint(f\"\\nSelected mode: {decision.mode.value.upper()}\")\nprint(f\"Confidence: {decision.confidence:.2f}\")\nprint(f\"Reasoning: {decision.mode_reasoning}\")\n\n# Compare with rule-based decision\nprint(\"\\n\" + \"-\" * 70)\nprint(\"COMPARISON: Rule-Based vs LLM-Enhanced\")\nprint(\"-\" * 70)\n\nrule_decision = mode_selector.select(\n    query=test_query,\n    memories=sample_memories,\n    patterns=[],\n)\n\nprint(f\"\\nRule-based mode:  {rule_decision.mode.value.upper()} ({rule_decision.reason})\")\nprint(f\"LLM-enhanced mode: {decision.mode.value.upper()} ({decision.mode_reasoning})\")\n\n# Show temperature adjustment impact\nprint(\"\\n\" + \"-\" * 70)\nprint(\"TEMPERATURE ADJUSTMENT IMPACT\")\nprint(\"-\" * 70)\n\nprint(\"\\nMemory | Rule-Based | LLM-Adjusted | Delta\")\nprint(\"-\" * 50)\nfor analysis in decision.memory_analyses:\n    mem = next((m for m in sample_memories if m.id == analysis.memory_id), None)\n    if mem:\n        delta = analysis.adjusted_temperature - analysis.original_temperature\n        direction = \"\u2191\" if delta > 0 else \"\u2193\" if delta < 0 else \"=\"\n        print(f\"{mem.memory_type.value[:8]:8} | {analysis.original_temperature:.3f}      | {analysis.adjusted_temperature:.3f}        | {direction} {abs(delta):.3f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9C: Agent Chat Continuity\n",
    "\n",
    "### The Concept\n",
    "\n",
    "Each LLM call is **stateless**\u2014the model has no memory of previous interactions. So how do AI agents maintain coherent, continuous conversations?\n",
    "\n",
    "The answer: **Store messages with labels** and reconstruct context from **hot tier memories + matched patterns**.\n",
    "\n",
    "### Memory Labels\n",
    "\n",
    "HGM uses three labels to categorize what's stored in hot memory:\n",
    "\n",
    "| Label | What It Stores | Purpose |\n",
    "|-------|----------------|--------|\n",
    "| `USER_QUERY` | User's exact message | Track what user asked |\n",
    "| `AGENT_THOUGHT` | Agent's reasoning/response | Track decisions made |\n",
    "| `PATTERN` | Learned strategy reference | Apply known solutions |\n",
    "\n",
    "### Why Labels Matter\n",
    "\n",
    "| Without Labels | With Labels |\n",
    "|---------------|------------|\n",
    "| All memories treated equally | Context type preserved |\n",
    "| Can't distinguish user vs agent | Clear conversation flow |\n",
    "| Patterns mixed with chat | Strategies clearly marked |\n",
    "| Harder to prioritize recall | Intelligent filtering |\n",
    "\n",
    "### The Context Assembly Formula\n",
    "\n",
    "For each user message, HGM assembles context with labels:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    AGENT CONTEXT WINDOW                      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  [SYSTEM PROMPT]                                             \u2502\n",
    "\u2502  [HOT MEMORIES]                                              \u2502\n",
    "\u2502     \u2022 [USER_QUERY] \"Deploy my Python app\"                   \u2502\n",
    "\u2502     \u2022 [AGENT_THOUGHT] \"User needs containerization\"         \u2502\n",
    "\u2502     \u2022 [USER_QUERY] \"Use Kubernetes\"                         \u2502\n",
    "\u2502     \u2022 [AGENT_THOUGHT] \"Switching to K8s approach\"           \u2502\n",
    "\u2502  [MATCHED PATTERNS]                                          \u2502\n",
    "\u2502     \u2022 [PATTERN] kubernetes_deployment \u2192 k8s manifest        \u2502\n",
    "\u2502  [EPISODE CONTEXT] Topic: deployment, Entities: k8s, python \u2502\n",
    "\u2502  [USER MESSAGE] Current query                                \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### Turn-Based Continuity Flow\n",
    "\n",
    "```\n",
    "TURN 1: User \u2192 \"Help me deploy my app\"\n",
    "   \u250c\u2500 STORE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "   \u2502  [USER_QUERY] \"Help me deploy my app\"               \u2502\n",
    "   \u2502  [AGENT_THOUGHT] \"User needs deployment guidance\"   \u2502\n",
    "   \u2502  [PATTERN] deployment \u2192 docker_strategy             \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "   Agent \u2192 \"I'll help you deploy using Docker...\"\n",
    "\n",
    "TURN 2: User \u2192 \"Use Kubernetes instead\"\n",
    "   \u250c\u2500 RECALL from HOT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "   \u2502  [USER_QUERY] \"Help me deploy my app\"  \u2190 Turn 1     \u2502\n",
    "   \u2502  [AGENT_THOUGHT] \"User needs deployment guidance\"   \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "   Agent knows we're discussing deployment!\n",
    "\n",
    "TURN 3: User \u2192 \"What about secrets?\"  (no explicit K8s mention)\n",
    "   \u250c\u2500 RECALL from HOT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "   \u2502  [USER_QUERY] \"Use Kubernetes instead\"  \u2190 Turn 2    \u2502\n",
    "   \u2502  [PATTERN] kubernetes \u2192 active context              \u2502\n",
    "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "   Agent infers \"secrets\" = Kubernetes secrets!\n",
    "```\n",
    "\n",
    "### What the Code Demonstrates\n",
    "\n",
    "1. **MemoryLabel enum** - Classification for context types\n",
    "2. **Labeled storage** - Each memory tagged with its type\n",
    "3. **Context assembly** - Labels enable intelligent recall\n",
    "4. **Successive messages** - Short follow-ups without breaking context\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# AGENT CHAT CONTINUITY WITH LABELS\n",
    "# =============================================================================\n",
    "\n",
    "class MemoryLabel(str, Enum):\n",
    "    \"\"\"\n",
    "    Labels for categorizing memories in the hot tier.\n",
    "    These enable intelligent context assembly and filtering.\n",
    "    \"\"\"\n",
    "    USER_QUERY = \"user_query\"       # User's message\n",
    "    AGENT_THOUGHT = \"agent_thought\" # Agent's reasoning/response\n",
    "    PATTERN = \"pattern\"             # Matched strategy reference\n",
    "\n",
    "@dataclass\n",
    "class LabeledMemory:\n",
    "    \"\"\"A memory with an explicit label for context type.\"\"\"\n",
    "    label: MemoryLabel\n",
    "    content: str\n",
    "    turn_number: int\n",
    "    timestamp: datetime\n",
    "    embedding: np.ndarray = field(default_factory=lambda: np.zeros(384))\n",
    "    temperature: float = 1.0\n",
    "    \n",
    "    def to_context_string(self) -> str:\n",
    "        \"\"\"Format for inclusion in LLM context.\"\"\"\n",
    "        return f\"[{self.label.value.upper()}] {self.content}\"\n",
    "\n",
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    \"\"\"A single turn in a conversation with labeled memories.\"\"\"\n",
    "    turn_number: int\n",
    "    user_message: str\n",
    "    agent_response: str\n",
    "    memories_recalled: list[str] = field(default_factory=list)\n",
    "    patterns_matched: list[str] = field(default_factory=list)\n",
    "\n",
    "class ChatAgent:\n",
    "    \"\"\"\n",
    "    Chat agent demonstrating continuity via labeled hot memory + patterns.\n",
    "    \n",
    "    Key innovation: Each memory is labeled (USER_QUERY, AGENT_THOUGHT, PATTERN)\n",
    "    enabling intelligent context reconstruction for successive messages.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str):\n",
    "        self.agent_id = agent_id\n",
    "        self.context = create_agent(agent_id, \"assistant\")\n",
    "        self.hot_tier = HotTier(agent_id, max_tokens=4000)\n",
    "        self.pattern_graph = graph  # Use existing pattern graph\n",
    "        self.labeled_memories: list[LabeledMemory] = []\n",
    "        self.conversation_history: list[ConversationTurn] = []\n",
    "        self.turn_counter = 0\n",
    "        self.scorer = TemperatureScorer()\n",
    "    \n",
    "    def store_labeled_memory(\n",
    "        self,\n",
    "        label: MemoryLabel,\n",
    "        content: str,\n",
    "        keywords: list[str] = None\n",
    "    ) -> LabeledMemory:\n",
    "        \"\"\"\n",
    "        Store a memory with an explicit label.\n",
    "        This is the KEY function for labeled context management.\n",
    "        \"\"\"\n",
    "        # Generate embedding\n",
    "        np.random.seed(hash(content) % 2**32)\n",
    "        embedding = np.random.randn(384).astype(np.float32)\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        # Create labeled memory\n",
    "        labeled_mem = LabeledMemory(\n",
    "            label=label,\n",
    "            content=content,\n",
    "            turn_number=self.turn_counter,\n",
    "            timestamp=datetime.now(),\n",
    "            embedding=embedding,\n",
    "            temperature=1.0,  # Fresh memories are hot\n",
    "        )\n",
    "        self.labeled_memories.append(labeled_mem)\n",
    "        \n",
    "        # Also store in hot tier for fast retrieval\n",
    "        hot_mem = create_memory(\n",
    "            content=f\"[{label.value.upper()}] {content}\",\n",
    "            memory_type=MemoryType.EPISODIC,\n",
    "            hierarchy_path=f\"conversation/{label.value}\",\n",
    "            entity_ids=keywords or [],\n",
    "            hours_ago=0,\n",
    "            access_count=1,\n",
    "        )\n",
    "        self.hot_tier.put(to_hot_memory(hot_mem))\n",
    "        \n",
    "        return labeled_mem\n",
    "    \n",
    "    def recall_labeled_context(self, query: str, limit: int = 8) -> list[LabeledMemory]:\n",
    "        \"\"\"\n",
    "        Recall relevant labeled memories for context assembly.\n",
    "        Returns memories sorted by relevance, preserving labels.\n",
    "        \"\"\"\n",
    "        if not self.labeled_memories:\n",
    "            return []\n",
    "        \n",
    "        # Generate query embedding\n",
    "        np.random.seed(hash(query) % 2**32)\n",
    "        query_emb = np.random.randn(384).astype(np.float32)\n",
    "        query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "        \n",
    "        # Score memories by recency + relevance\n",
    "        scored = []\n",
    "        for mem in self.labeled_memories:\n",
    "            # Recency: recent memories score higher\n",
    "            recency = 1.0 - (self.turn_counter - mem.turn_number) * 0.1\n",
    "            recency = max(0.1, recency)\n",
    "            \n",
    "            # Relevance: cosine similarity to query\n",
    "            relevance = float(np.dot(query_emb, mem.embedding))\n",
    "            \n",
    "            # Combined score\n",
    "            score = 0.4 * recency + 0.6 * relevance\n",
    "            scored.append((score, mem))\n",
    "        \n",
    "        # Sort by score descending, return top memories\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [mem for _, mem in scored[:limit]]\n",
    "    \n",
    "    def assemble_context(self, query: str) -> dict:\n",
    "        \"\"\"\n",
    "        Assemble full context from labeled memories + patterns.\n",
    "        This reconstructs conversation state for each turn.\n",
    "        \"\"\"\n",
    "        keywords = self.pattern_graph.extract_keywords(query)\n",
    "        \n",
    "        # 1. Get labeled memories from hot tier\n",
    "        labeled_mems = self.recall_labeled_context(query)\n",
    "        \n",
    "        # Separate by label for structured context\n",
    "        user_queries = [m for m in labeled_mems if m.label == MemoryLabel.USER_QUERY]\n",
    "        agent_thoughts = [m for m in labeled_mems if m.label == MemoryLabel.AGENT_THOUGHT]\n",
    "        \n",
    "        # 2. Get matched patterns\n",
    "        patterns = self.pattern_graph.find_patterns(keywords, limit=3)\n",
    "        pattern_strategies = [\n",
    "            f\"{p.trigger}: {p.strategy}\" \n",
    "            for p in patterns if p.relevance > 0.5\n",
    "        ]\n",
    "        \n",
    "        # 3. Build context string with labels\n",
    "        context_items = []\n",
    "        for mem in labeled_mems:\n",
    "            context_items.append(mem.to_context_string())\n",
    "        \n",
    "        for strategy in pattern_strategies:\n",
    "            context_items.append(f\"[PATTERN] {strategy}\")\n",
    "        \n",
    "        return {\n",
    "            \"labeled_memories\": labeled_mems,\n",
    "            \"user_queries\": [m.content for m in user_queries],\n",
    "            \"agent_thoughts\": [m.content for m in agent_thoughts],\n",
    "            \"patterns\": pattern_strategies,\n",
    "            \"keywords\": keywords,\n",
    "            \"context_string\": \"\\n\".join(context_items),\n",
    "        }\n",
    "    \n",
    "    def process_message(self, user_message: str) -> tuple[str, dict]:\n",
    "        \"\"\"\n",
    "        Process a user message with full labeled context management.\n",
    "        \n",
    "        Steps:\n",
    "        1. Assemble context from previous labeled memories\n",
    "        2. Store user message as [USER_QUERY]\n",
    "        3. Generate response (simulated)\n",
    "        4. Store response as [AGENT_THOUGHT]\n",
    "        5. Store any matched patterns as [PATTERN]\n",
    "        \"\"\"\n",
    "        self.turn_counter += 1\n",
    "        \n",
    "        # Assemble context from previous turns\n",
    "        context = self.assemble_context(user_message)\n",
    "        keywords = context[\"keywords\"]\n",
    "        \n",
    "        # Store user message with label\n",
    "        self.store_labeled_memory(\n",
    "            MemoryLabel.USER_QUERY,\n",
    "            user_message,\n",
    "            keywords\n",
    "        )\n",
    "        \n",
    "        # Update agent focus\n",
    "        np.random.seed(hash(user_message) % 2**32)\n",
    "        focus_emb = np.random.randn(384).astype(np.float32)\n",
    "        self.context.update_focus(\n",
    "            embedding=focus_emb / np.linalg.norm(focus_emb),\n",
    "            entities=set(keywords),\n",
    "        )\n",
    "        \n",
    "        # Generate response (simulated LLM call)\n",
    "        response, thought = self._generate_response(user_message, context)\n",
    "        \n",
    "        # Store agent thought with label\n",
    "        self.store_labeled_memory(\n",
    "            MemoryLabel.AGENT_THOUGHT,\n",
    "            thought,\n",
    "            keywords\n",
    "        )\n",
    "        \n",
    "        # Store matched patterns as labeled memories\n",
    "        for pattern in context[\"patterns\"][:1]:  # Top pattern only\n",
    "            self.store_labeled_memory(\n",
    "                MemoryLabel.PATTERN,\n",
    "                pattern,\n",
    "                keywords\n",
    "            )\n",
    "        \n",
    "        # Record conversation turn\n",
    "        self.conversation_history.append(ConversationTurn(\n",
    "            turn_number=self.turn_counter,\n",
    "            user_message=user_message,\n",
    "            agent_response=response,\n",
    "            memories_recalled=[m.to_context_string() for m in context[\"labeled_memories\"][:3]],\n",
    "            patterns_matched=context[\"patterns\"][:2],\n",
    "        ))\n",
    "        \n",
    "        return response, context\n",
    "    \n",
    "    def _generate_response(self, query: str, context: dict) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Simulate response generation. Returns (response, thought).\n",
    "        In production, this calls the LLM with assembled context.\n",
    "        \"\"\"\n",
    "        # Check for pattern-based response\n",
    "        if context[\"patterns\"]:\n",
    "            strategy = context[\"patterns\"][0].split(\": \")[1] if \": \" in context[\"patterns\"][0] else context[\"patterns\"][0]\n",
    "            thought = f\"Using pattern strategy: {strategy}\"\n",
    "            return f\"Based on our approach: {strategy}\", thought\n",
    "        \n",
    "        # Check for continuity from previous user queries\n",
    "        if context[\"user_queries\"]:\n",
    "            prev_query = context[\"user_queries\"][0]\n",
    "            thought = f\"Continuing from previous query about: {prev_query[:40]}\"\n",
    "            return f\"Building on what we discussed: {prev_query[:50]}...\", thought\n",
    "        \n",
    "        thought = \"Starting new conversation thread\"\n",
    "        return \"I'll help you with that. What aspect would you like to explore?\", thought\n",
    "    \n",
    "    def show_memory_state(self):\n",
    "        \"\"\"Display current labeled memory state.\"\"\"\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"LABELED MEMORY STATE (Turn {self.turn_counter})\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        \n",
    "        for label in MemoryLabel:\n",
    "            mems = [m for m in self.labeled_memories if m.label == label]\n",
    "            print(f\"\\n[{label.value.upper()}] ({len(mems)} memories)\")\n",
    "            for m in mems[-3:]:  # Show last 3 of each type\n",
    "                print(f\"  Turn {m.turn_number}: {m.content[:50]}...\" if len(m.content) > 50 else f\"  Turn {m.turn_number}: {m.content}\")\n",
    "\n",
    "# Create chat agent\n",
    "chat_agent = ChatAgent(\"demo-chat-agent\")\n",
    "print(\"ChatAgent with labeled memory created\")\n",
    "print(f\"Hot tier capacity: {chat_agent.hot_tier.max_tokens} tokens\")\n",
    "print(f\"\\nMemory labels available:\")\n",
    "for label in MemoryLabel:\n",
    "    print(f\"  \u2022 {label.value.upper()}: {label.name}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# TURN-BASED CONTINUITY DEMO: SUCCESSIVE MESSAGES\n",
    "# =============================================================================\n",
    "# This demonstrates how users can send short follow-up messages\n",
    "# without breaking context - the key benefit of labeled memories.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TURN-BASED CONTINUITY DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWatch how the agent maintains context across successive short messages.\")\n",
    "print(\"Each message builds on previous context without explicit repetition.\\n\")\n",
    "\n",
    "# Simulate a multi-turn conversation with short follow-ups\n",
    "conversation = [\n",
    "    \"How do I deploy my Python application?\",  # Turn 1: Initial context\n",
    "    \"Can you show me the Docker approach?\",     # Turn 2: Builds on \"deploy\"\n",
    "    \"What about environment variables?\",        # Turn 3: Short follow-up\n",
    "    \"And secrets?\",                             # Turn 4: Very short - relies on context\n",
    "    \"Show me the full Dockerfile\",              # Turn 5: Assumes Docker context\n",
    "]\n",
    "\n",
    "for i, message in enumerate(conversation, 1):\n",
    "    print(f\"\\n{'\u2500' * 70}\")\n",
    "    print(f\"TURN {i}\")\n",
    "    print(f\"{'\u2500' * 70}\")\n",
    "    print(f\"\\n\ud83d\udc64 User: {message}\")\n",
    "    \n",
    "    response, context = chat_agent.process_message(message)\n",
    "    \n",
    "    print(f\"\\n\ud83e\udd16 Agent: {response}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Context Assembly (with labels):\")\n",
    "    print(f\"   Keywords extracted: {context['keywords']}\")\n",
    "    \n",
    "    # Show recalled labeled memories\n",
    "    if context['labeled_memories']:\n",
    "        print(f\"   \\n   Recalled memories ({len(context['labeled_memories'])}):\")\n",
    "        for mem in context['labeled_memories'][:4]:\n",
    "            label_str = mem.label.value.upper()\n",
    "            content_preview = mem.content[:45] + \"...\" if len(mem.content) > 45 else mem.content\n",
    "            print(f\"      [{label_str}] Turn {mem.turn_number}: {content_preview}\")\n",
    "    \n",
    "    # Show patterns\n",
    "    if context['patterns']:\n",
    "        print(f\"   \\n   Matched patterns:\")\n",
    "        for pat in context['patterns'][:2]:\n",
    "            print(f\"      [PATTERN] {pat[:55]}...\" if len(pat) > 55 else f\"      [PATTERN] {pat}\")\n",
    "    \n",
    "    print(f\"\\n   Hot tier: {chat_agent.hot_tier.stats()['memory_count']} memories\")\n",
    "\n",
    "# Show final memory state\n",
    "chat_agent.show_memory_state()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONTINUITY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal turns: {len(conversation)}\")\n",
    "print(f\"Total labeled memories: {len(chat_agent.labeled_memories)}\")\n",
    "print(f\"\\nBreakdown by label:\")\n",
    "for label in MemoryLabel:\n",
    "    count = len([m for m in chat_agent.labeled_memories if m.label == label])\n",
    "    print(f\"   {label.value.upper()}: {count}\")\n",
    "\n",
    "print(\"\\n\u2705 Key Observations:\")\n",
    "print(\"   \u2022 Short messages like 'And secrets?' work because of context\")\n",
    "print(\"   \u2022 [USER_QUERY] labels track what user asked\")\n",
    "print(\"   \u2022 [AGENT_THOUGHT] labels preserve reasoning\")\n",
    "print(\"   \u2022 [PATTERN] labels apply learned strategies\")\n",
    "print(\"   \u2022 Agent doesn't need explicit topic repetition\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CROSS-SESSION CONTINUITY (Pattern Persistence)\n",
    "# =============================================================================\n",
    "# Labels also enable cross-session memory:\n",
    "# - [USER_QUERY] and [AGENT_THOUGHT] are session-scoped (hot tier)\n",
    "# - [PATTERN] can persist across sessions (cold tier)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CROSS-SESSION CONTINUITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "WITHIN A SESSION:\n",
    "  \u2022 Hot tier maintains [USER_QUERY] and [AGENT_THOUGHT] memories\n",
    "  \u2022 Recent messages stay accessible via labeled recall\n",
    "  \u2022 Context builds naturally turn by turn\n",
    "\n",
    "ACROSS SESSIONS:\n",
    "  \u2022 [PATTERN] memories persist in cold tier\n",
    "  \u2022 Learned strategies survive session restart\n",
    "  \u2022 User preferences become permanent patterns\n",
    "\n",
    "EXAMPLE:\n",
    "\u250c\u2500 Session 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  User: \"I prefer kubectl over helm for Kubernetes\"            \u2502\n",
    "\u2502  \u2193                                                             \u2502\n",
    "\u2502  Store: [USER_QUERY] \"I prefer kubectl over helm\"             \u2502\n",
    "\u2502  Store: [AGENT_THOUGHT] \"User prefers CLI over helm charts\"   \u2502\n",
    "\u2502  Create: [PATTERN] kubernetes_preference \u2192 \"use kubectl\"      \u2502\n",
    "\u2502  \u2193                                                             \u2502\n",
    "\u2502  Promote [PATTERN] to COLD tier for persistence               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "\u250c\u2500 Session 2 (next day) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  User: \"How do I check my deployments?\"                       \u2502\n",
    "\u2502  \u2193                                                             \u2502\n",
    "\u2502  Recall from COLD: [PATTERN] kubernetes_preference \u2192 kubectl  \u2502\n",
    "\u2502  \u2193                                                             \u2502\n",
    "\u2502  Agent response: \"Use kubectl get deployments...\"             \u2502\n",
    "\u2502  (Agent remembers user prefers kubectl!)                       \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\"\"\")\n",
    "\n",
    "# Demonstrate pattern creation for preference\n",
    "print(\"Creating a preference pattern for cross-session persistence...\")\n",
    "graph.add_pattern(\n",
    "    \"pat_user_pref_kubectl\",\n",
    "    \"User prefers kubectl\",\n",
    "    \"When discussing Kubernetes, prefer kubectl commands over helm charts\",\n",
    "    effectiveness=0.9,\n",
    ")\n",
    "graph.add_entity(\"kubectl\")\n",
    "graph.add_entity(\"preference\")\n",
    "graph.link_entity_to_pattern(\"kubectl\", \"pat_user_pref_kubectl\", 0.9)\n",
    "graph.link_entity_to_pattern(\"preference\", \"pat_user_pref_kubectl\", 0.7)\n",
    "\n",
    "print(\"Pattern created: [PATTERN] kubernetes_preference\")\n",
    "print(\"This pattern will persist across sessions in cold tier.\")\n",
    "print(f\"\\nPattern graph now has {graph.stats()['patterns']} patterns\")\n",
    "\n",
    "print(\"\\n\" + \"\u2500\" * 70)\n",
    "print(\"KEY INSIGHT: Labels Enable Smart Persistence\")\n",
    "print(\"\u2500\" * 70)\n",
    "print(\"\"\"\n",
    "  [USER_QUERY]    \u2192 Ephemeral (hot tier only)\n",
    "  [AGENT_THOUGHT] \u2192 Ephemeral (hot tier only)  \n",
    "  [PATTERN]       \u2192 Persistent (promoted to cold tier)\n",
    "\n",
    "This separation allows the system to:\n",
    "  \u2713 Keep conversations fast (hot tier for recent context)\n",
    "  \u2713 Learn permanently (patterns persist in cold tier)\n",
    "  \u2713 Forget appropriately (old queries expire naturally)\n",
    "\"\"\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 10: Complete Pipeline Demo\n\n### The Concept\n\nThis section ties everything together into a **complete query processing pipeline**. Watch how a single query flows through all HGM components from input to response decision.\n\n### The End-to-End Flow\n\n```\n                           USER QUERY\n                               \u2502\n                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  STEP 1: KEYWORD EXTRACTION                                 \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                \u2502\n\u2502  Input:  \"How do I deploy my Python application?\"           \u2502\n\u2502  Output: [\"deploy\", \"python\", \"application\"]                \u2502\n\u2502  Algorithm: Regex tokenization + stopword filtering         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  STEP 2: PATTERN GRAPH LOOKUP                               \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                              \u2502\n\u2502  Input:  Keywords from Step 1                               \u2502\n\u2502  Output: Pattern matches with relevance scores              \u2502\n\u2502  Algorithm: Graph traversal via entity\u2192pattern edges        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  STEP 3: MEMORY RECALL (Agentic RAG)                        \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                       \u2502\n\u2502  Input:  Query embedding + focus entities                   \u2502\n\u2502  Output: Relevant memories + tier movements                 \u2502\n\u2502  Algorithm: Parallel tier search + temperature scoring      \u2502\n\u2502             Automatic promotion/demotion                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  STEP 4: PATTERN SCORING                                    \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                   \u2502\n\u2502  Input:  Pattern matches + query embedding                  \u2502\n\u2502  Output: Ranked patterns with detailed breakdowns           \u2502\n\u2502  Algorithm: Multi-factor weighted scoring                   \u2502\n\u2502             (semantic + keyword + topic + structure)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  STEP 5: MODE SELECTION                                     \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                    \u2502\n\u2502  Input:  Memories + scored patterns + query                 \u2502\n\u2502  Output: Response mode (FAST/AGENT/PATTERN_DIRECT/WORKFLOW) \u2502\n\u2502  Algorithm: Priority-based decision tree                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                          RESPONSE GENERATION\n                     (Mode-specific execution)\n```\n\n### Why This Architecture Works\n\n1. **Parallel Paths**: Pattern lookup and memory recall can run simultaneously\n2. **Progressive Filtering**: Each step reduces the search space\n3. **Multi-Signal Decisions**: No single component makes the final call\n4. **Adaptive Behavior**: Tier movements improve future queries\n\n### What the Code Demonstrates\n\n1. **HGMPipeline** class - Orchestrates all components\n2. **process_query()** - Full trace through all steps\n3. **Step-by-step output** - See each component's contribution\n4. **Two example queries** - Different paths through the system\n\n### Key Observations to Note\n\n- **Same query, different context**: Results change based on agent focus\n- **Promotion effects**: Relevant cold memories get promoted\n- **Mode varies**: Simple queries \u2192 FAST, complex \u2192 WORKFLOW\n- **Pattern shortcuts**: High-confidence patterns bypass LLM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGMPipeline:\n",
    "    \"\"\"Complete HGM query processing pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = graph\n",
    "        self.tiers = tiers\n",
    "        self.pattern_scorer = pattern_scorer\n",
    "        self.mode_selector = mode_selector\n",
    "        self.temp_scorer = scorer\n",
    "    \n",
    "    def process_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        agent_context: AgentContext,\n",
    "    ) -> dict:\n",
    "        \"\"\"Process a query through the full pipeline.\"\"\"\n",
    "        results = {\"query\": query, \"steps\": []}\n",
    "        \n",
    "        # Step 1: Extract keywords\n",
    "        keywords = self.graph.extract_keywords(query)\n",
    "        results[\"steps\"].append({\n",
    "            \"step\": \"1. Keyword Extraction\",\n",
    "            \"keywords\": keywords,\n",
    "        })\n",
    "        \n",
    "        # Step 2: Pattern graph lookup\n",
    "        pattern_matches = self.graph.find_patterns(keywords, limit=5)\n",
    "        results[\"steps\"].append({\n",
    "            \"step\": \"2. Pattern Graph Lookup\",\n",
    "            \"matches_found\": len(pattern_matches),\n",
    "            \"top_pattern\": pattern_matches[0].trigger[:40] if pattern_matches else None,\n",
    "        })\n",
    "        \n",
    "        # Step 3: Memory recall with promotion\n",
    "        np.random.seed(hash(query) % 2**32)\n",
    "        query_emb = np.random.randn(384).astype(np.float32)\n",
    "        query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "        \n",
    "        recalled, placements = self.tiers.recall(\n",
    "            query_embedding=query_emb,\n",
    "            focus_entities=set(keywords),\n",
    "            limit=5,\n",
    "        )\n",
    "        results[\"steps\"].append({\n",
    "            \"step\": \"3. Memory Recall\",\n",
    "            \"recalled\": len(recalled),\n",
    "            \"promotions\": sum(1 for p in placements if p.promoted),\n",
    "            \"demotions\": sum(1 for p in placements if p.demoted),\n",
    "        })\n",
    "        \n",
    "        # Step 4: Score patterns\n",
    "        scored_patterns = []\n",
    "        for pm in pattern_matches:\n",
    "            pattern_dict = {\"id\": pm.pattern_id, \"trigger\": pm.trigger, \"strategy\": pm.strategy}\n",
    "            np.random.seed(hash(pm.trigger) % 2**32)\n",
    "            pattern_emb = np.random.randn(384).astype(np.float32)\n",
    "            pattern_emb = pattern_emb / np.linalg.norm(pattern_emb)\n",
    "            \n",
    "            scored = self.pattern_scorer.score(\n",
    "                query=query,\n",
    "                pattern=pattern_dict,\n",
    "                query_embedding=query_emb,\n",
    "                pattern_embedding=pattern_emb,\n",
    "            )\n",
    "            scored_patterns.append(scored)\n",
    "        \n",
    "        results[\"steps\"].append({\n",
    "            \"step\": \"4. Pattern Scoring\",\n",
    "            \"scored\": len(scored_patterns),\n",
    "            \"top_score\": f\"{max(p.relevance for p in scored_patterns):.3f}\" if scored_patterns else \"N/A\",\n",
    "        })\n",
    "        \n",
    "        # Step 5: Mode selection\n",
    "        decision = self.mode_selector.select(\n",
    "            query=query,\n",
    "            memories=recalled,\n",
    "            patterns=scored_patterns,\n",
    "            conversation_turns=agent_context.turn_count,\n",
    "        )\n",
    "        results[\"steps\"].append({\n",
    "            \"step\": \"5. Mode Selection\",\n",
    "            \"mode\": decision.mode.value,\n",
    "            \"reason\": decision.reason,\n",
    "            \"confidence\": f\"{decision.confidence:.2f}\",\n",
    "        })\n",
    "        \n",
    "        # Update agent context\n",
    "        agent_context.update_focus(\n",
    "            embedding=query_emb,\n",
    "            entities=set(keywords),\n",
    "        )\n",
    "        \n",
    "        results[\"decision\"] = decision\n",
    "        return results\n",
    "\n",
    "pipeline = HGMPipeline()\n",
    "print(\"HGMPipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline with test queries\n",
    "\n",
    "test_queries = [\n",
    "    \"How do I deploy my Python application?\",\n",
    "    \"Tell me about machine learning training\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FULL PIPELINE DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    result = pipeline.process_query(query, researcher)\n",
    "    \n",
    "    for step in result[\"steps\"]:\n",
    "        print(f\"\\n{step['step']}:\")\n",
    "        for k, v in step.items():\n",
    "            if k != \"step\":\n",
    "                print(f\"  {k}: {v}\")\n",
    "    \n",
    "    print(f\"\\n>>> FINAL DECISION: {result['decision'].mode.value.upper()}\")\n",
    "    print(f\"    Reason: {result['decision'].reason}\")\n",
    "    if result['decision'].direct_response:\n",
    "        print(f\"    Response: {result['decision'].direct_response[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 11: Visualizations\n\n### The Concept\n\nVisualizations help us understand the **state of the system** at any point. Since this workshop avoids external dependencies, we use ASCII-based visualizations that work anywhere.\n\n### What These Visualizations Show\n\n#### 1. Temperature Distribution\nSee how memories are distributed across temperature zones:\n- Which memories are \"hot\" (actively relevant)?\n- Which are \"cold\" (archived)?\n- How does the distribution change after queries?\n\n#### 2. Tier Utilization\nMonitor the three-tier memory system:\n- Is the hot tier full? (May need larger budget)\n- Are memories being promoted effectively?\n- Is cold tier growing? (May need archival policy)\n\n#### 3. Pattern Graph Structure\nUnderstand the knowledge graph:\n- How many entities and patterns exist?\n- How connected is the graph?\n- Which entities link to which patterns?\n\n### Why Visualizations Matter\n\nIn production systems, these metrics help with:\n- **Capacity planning**: Hot tier token budget sizing\n- **Performance tuning**: Temperature threshold adjustment\n- **Debugging**: Why did a memory not get retrieved?\n- **Monitoring**: System health dashboards"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASCII visualizations (no matplotlib needed)\n",
    "\n",
    "def ascii_bar(value: float, width: int = 30, char: str = \"#\") -> str:\n",
    "    \"\"\"Create an ASCII progress bar.\"\"\"\n",
    "    filled = int(value * width)\n",
    "    return f\"[{char * filled}{' ' * (width - filled)}] {value:.1%}\"\n",
    "\n",
    "def visualize_temperatures(memories: list[Memory]):\n",
    "    \"\"\"Visualize temperature distribution.\"\"\"\n",
    "    print(\"\\nTemperature Distribution:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for m in sorted(memories, key=lambda x: x.temperature, reverse=True):\n",
    "        zone = scorer.get_zone(m.temperature)\n",
    "        bar = ascii_bar(m.temperature, 20)\n",
    "        print(f\"  {zone.value:8} {bar} {m.content[:25]}...\")\n",
    "\n",
    "def visualize_tiers(tier_system: ThreeTierMemory):\n",
    "    \"\"\"Visualize tier utilization.\"\"\"\n",
    "    stats = tier_system.stats()\n",
    "    total = stats[\"total\"] or 1\n",
    "    \n",
    "    print(\"\\nTier Utilization:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for tier_name in [\"hot\", \"warm\", \"cold\"]:\n",
    "        count = stats[tier_name]\n",
    "        pct = count / total\n",
    "        bar = ascii_bar(pct, 20)\n",
    "        print(f\"  {tier_name.upper():5} {bar} ({count} memories)\")\n",
    "\n",
    "def visualize_pattern_graph(g: PatternGraph):\n",
    "    \"\"\"Visualize pattern graph structure.\"\"\"\n",
    "    stats = g.stats()\n",
    "    \n",
    "    print(\"\\nPattern Graph Structure:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Entities: {stats['entities']}\")\n",
    "    print(f\"  Patterns: {stats['patterns']}\")\n",
    "    print(f\"  Edges:    {stats['edges']}\")\n",
    "    print(f\"  Queries:  {stats['queries']}\")\n",
    "    \n",
    "    print(\"\\n  Sample Entity -> Pattern Links:\")\n",
    "    for entity, links in list(g._entity_to_patterns.items())[:5]:\n",
    "        pattern_ids = [pid for pid, _ in links]\n",
    "        print(f\"    {entity} -> {pattern_ids}\")\n",
    "\n",
    "# Run visualizations\n",
    "visualize_temperatures(sample_memories)\n",
    "visualize_tiers(tiers)\n",
    "visualize_pattern_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 12: Summary & Exercises\n\n### Workshop Recap\n\nYou've learned the core concepts that power intelligent AI agent memory systems:\n\n| Concept | What You Learned | Key Algorithm |\n|---------|------------------|---------------|\n| **Memory Types** | SEMANTIC, EPISODIC, PROCEDURAL, EMOTIONAL classification | Cognitive science-based categorization |\n| **Agent State** | Focus tracking, episodes, sessions | Context engineering patterns |\n| **Temperature** | 5-factor scoring for relevance | Weighted linear combination with exponential decay |\n| **Hot Tier** | Working memory with token-based eviction | LRU eviction + vectorized similarity |\n| **Agentic RAG** | Active recall with promotion/demotion | Three-tier architecture with temperature thresholds |\n| **Pattern Graph** | Entity-pattern knowledge structure | Graph traversal with strength-weighted edges |\n| **Pattern Scoring** | Multi-factor relevance computation | Cosine similarity + Jaccard + structure matching |\n| **Mode Selection** | Response strategy routing | Priority-based decision tree |\n\n### Key Formulas\n\n**Temperature Scoring:**\n```\ntemperature = 0.30 \u00d7 recency + 0.15 \u00d7 frequency + 0.35 \u00d7 relevance + 0.15 \u00d7 entity_overlap + 0.05 \u00d7 agent_match\n\nwhere:\n  recency = 0.5^(hours_ago / 24)  // Exponential decay\n  frequency = access_count / max_count\n  relevance = (cosine_similarity + 1) / 2  // Normalized to [0,1]\n  entity_overlap = |memory_entities \u2229 focus_entities| / |focus_entities|\n```\n\n**Pattern Relevance Scoring:**\n```\nrelevance = 0.40 \u00d7 semantic + 0.15 \u00d7 keyword + 0.20 \u00d7 topic + 0.10 \u00d7 structure + 0.15 \u00d7 boost\n\nwhere:\n  semantic = cosine_similarity(query_emb, pattern_emb)\n  keyword = jaccard(query_keywords, pattern_keywords)\n  topic = min(1.0, |long_word_matches| \u00d7 0.5)\n  structure = 1.0 if same_question_pattern else 0.3 if both_have_patterns else 0.0\n```\n\n### Architecture Benefits\n\n| Benefit | How HGM Achieves It |\n|---------|---------------------|\n| **Speed** | Hot tier with SIMD-accelerated similarity (<1ms) |\n| **Intelligence** | Pattern graph enables learned response strategies |\n| **Adaptability** | Temperature-based self-organization |\n| **Scalability** | Three tiers handle different access patterns |\n| **Personalization** | Per-agent context and focus tracking |\n| **Efficiency** | Mode selection minimizes unnecessary LLM calls |\n\n### Comparison: HGM vs Traditional RAG\n\n| Aspect | Traditional RAG | HGM |\n|--------|-----------------|-----|\n| Retrieval | Static vector search | Dynamic three-tier search |\n| Organization | Fixed indexes | Self-organizing via temperature |\n| Learning | None | Pattern graph grows from interactions |\n| Routing | Always LLM | Mode selection (sometimes no LLM needed) |\n| Context | Global | Per-agent focus tracking |\n| Latency | Consistent | Tiered (hot=fast, cold=slower) |\n\n### Further Reading\n- **Cognitive Science**: Tulving's memory taxonomy, working memory models\n- **Vector Search**: FAISS, Qdrant, pgvector for production embeddings\n- **Graph Databases**: Neo4j, memgraph for scaled pattern graphs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HANDS-ON EXERCISES\n# ============================================================================\n# These exercises will help you internalize the concepts by modifying and\n# experimenting with the code. Uncomment and run each exercise.\n\nprint(\"\"\"\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                           WORKSHOP EXERCISES                                  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nEXERCISE 1: MEMORY TYPES AND TEMPERATURE\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGoal: Understand how memory type affects retrieval in different contexts.\n\nTask: Create a PROCEDURAL memory about error handling, then observe how its\ntemperature changes when you query with different focus entities.\n\nExpected Observation: The memory should get higher temperature when focus\nincludes \"error\" or \"handling\" entities.\n\n# Uncomment and run:\n# error_memory = create_memory(\n#     content=\"When encountering a connection error, first check network status, \"\n#             \"then verify credentials, finally retry with exponential backoff.\",\n#     memory_type=MemoryType.PROCEDURAL,\n#     hierarchy_path=\"ops/error_handling\",\n#     entity_ids=[\"error\", \"connection\", \"retry\", \"backoff\"],\n#     hours_ago=12,\n#     access_count=3,\n# )\n#\n# # Score with error-related focus\n# np.random.seed(hash(\"error handling debugging\") % 2**32)\n# error_focus = np.random.randn(384).astype(np.float32)\n# error_focus = error_focus / np.linalg.norm(error_focus)\n# error_entities = {\"error\", \"debugging\", \"fix\", \"handling\"}\n#\n# temp_error = scorer.compute(error_memory, error_focus, error_entities)\n# print(f\"Temperature with error focus: {temp_error:.3f}\")\n#\n# # Score with unrelated focus (cooking)\n# np.random.seed(hash(\"cooking recipes food\") % 2**32)\n# cooking_focus = np.random.randn(384).astype(np.float32)\n# cooking_focus = cooking_focus / np.linalg.norm(cooking_focus)\n# cooking_entities = {\"cooking\", \"recipe\", \"food\"}\n#\n# temp_cooking = scorer.compute(error_memory, cooking_focus, cooking_entities)\n# print(f\"Temperature with cooking focus: {temp_cooking:.3f}\")\n# print(f\"Difference: {temp_error - temp_cooking:.3f}\")\n\n\nEXERCISE 2: EPISODE BOUNDARIES AND TOPIC DRIFT\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGoal: Understand how episodes help track topic changes in conversations.\n\nTask: Simulate a conversation that starts with ML, then switches to cooking.\nMeasure the entity drift between episodes.\n\nExpected Observation: Large entity drift indicates topic change, which should\ntrigger a new episode.\n\n# Uncomment and run:\n# def entity_drift(entities_a: set, entities_b: set) -> float:\n#     '''Measure how different two entity sets are (0 = same, 1 = completely different).'''\n#     if not entities_a and not entities_b:\n#         return 0.0\n#     intersection = len(entities_a & entities_b)\n#     union = len(entities_a | entities_b)\n#     return 1.0 - (intersection / union) if union > 0 else 0.0\n#\n# # Episode 1: Machine Learning discussion\n# ml_agent = create_agent(\"research-agent\", \"researcher\")\n# ml_agent.update_focus(\n#     entities={\"neural_network\", \"training\", \"gradient_descent\", \"pytorch\"},\n#     hierarchy_path=\"research/ml/deep_learning\",\n# )\n# ml_agent.start_episode(\"Discussing neural network training\")\n# ml_entities = ml_agent.focus_entities.copy()\n#\n# # Episode 2: Cooking discussion\n# cooking_entities = {\"recipe\", \"ingredients\", \"cooking\", \"temperature\"}\n#\n# # Measure drift\n# drift = entity_drift(ml_entities, cooking_entities)\n# print(f\"Entity drift: {drift:.3f}\")\n# print(f\"Should trigger new episode: {drift > 0.7}\")  # Threshold for new episode\n\n\nEXERCISE 3: PATTERN GRAPH EXTENSION\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGoal: Learn how to add domain-specific patterns to the graph.\n\nTask: Add patterns for a domain you're interested in (e.g., DevOps, data science,\ncooking). Test that they can be retrieved via keyword matching.\n\n# Uncomment and run:\n# # Add your own pattern\n# graph.add_pattern(\n#     \"pat_kubernetes\",\n#     \"How to deploy to Kubernetes?\",\n#     \"Use kubectl apply -f deployment.yaml or helm install for complex deployments.\",\n#     effectiveness=0.85,\n# )\n#\n# # Link keywords\n# for keyword in [\"kubernetes\", \"k8s\", \"deploy\", \"kubectl\", \"helm\", \"pod\"]:\n#     graph.add_entity(keyword)\n#     graph.link_entity_to_pattern(keyword, \"pat_kubernetes\", strength=0.85)\n#\n# # Test retrieval\n# test_query = \"How do I deploy my app to k8s?\"\n# keywords = graph.extract_keywords(test_query)\n# matches = graph.find_patterns(keywords, limit=3)\n# print(f\"Query: {test_query}\")\n# print(f\"Keywords: {keywords}\")\n# for m in matches:\n#     print(f\"  [{m.relevance:.3f}] {m.trigger}\")\n\n\nEXERCISE 4: TEMPERATURE WEIGHT TUNING\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGoal: Understand how weight changes affect temperature computation.\n\nTask: Experiment with different TemperatureConfig weights. What happens when\nyou increase weight_recency vs weight_relevance?\n\n# Uncomment and run:\n# # Default weights\n# default_config = TemperatureConfig()\n# default_scorer = TemperatureScorer(default_config)\n#\n# # Recency-heavy config (prioritizes recent memories)\n# recency_config = TemperatureConfig(\n#     weight_recency=0.50,  # Increased from 0.30\n#     weight_frequency=0.10,\n#     weight_relevance=0.25,  # Decreased from 0.35\n#     weight_entity=0.10,\n#     weight_agent=0.05,\n# )\n# recency_scorer = TemperatureScorer(recency_config)\n#\n# # Relevance-heavy config (prioritizes semantic match)\n# relevance_config = TemperatureConfig(\n#     weight_recency=0.15,  # Decreased\n#     weight_frequency=0.10,\n#     weight_relevance=0.55,  # Increased significantly\n#     weight_entity=0.15,\n#     weight_agent=0.05,\n# )\n# relevance_scorer = TemperatureScorer(relevance_config)\n#\n# # Compare on same memory\n# test_memory = sample_memories[2]  # The old deployment memory\n# print(f\"Memory: {test_memory.content[:50]}...\")\n# print(f\"Age: {(utcnow().timestamp() - test_memory.accessed_at) / 3600:.1f} hours\")\n# print()\n# print(f\"Default scorer:   {default_scorer.compute(test_memory, python_focus, python_entities):.3f}\")\n# print(f\"Recency-heavy:    {recency_scorer.compute(test_memory, python_focus, python_entities):.3f}\")\n# print(f\"Relevance-heavy:  {relevance_scorer.compute(test_memory, python_focus, python_entities):.3f}\")\n\n\nEXERCISE 5: MODE SELECTOR ENHANCEMENT\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGoal: Understand mode selection by adding a new mode.\n\nTask: Add a CLARIFICATION mode that triggers when the query is ambiguous\n(e.g., very short, contains \"?\" but no clear topic).\n\n# Uncomment and run:\n# class EnhancedResponseMode(str, Enum):\n#     FAST = \"fast\"\n#     AGENT = \"agent\"\n#     PATTERN_DIRECT = \"pattern\"\n#     WORKFLOW = \"workflow\"\n#     CLARIFICATION = \"clarification\"  # NEW!\n#\n# class EnhancedModeSelector(ModeSelector):\n#     def select(self, query, memories=None, patterns=None, conversation_turns=0):\n#         # Check for ambiguous queries first\n#         if self._is_ambiguous(query):\n#             return ModeDecision(\n#                 mode=EnhancedResponseMode.CLARIFICATION,\n#                 reason=\"ambiguous_query\",\n#                 confidence=0.7,\n#                 has_memories=bool(memories),\n#                 has_patterns=bool(patterns),\n#             )\n#         # Fall back to parent logic\n#         return super().select(query, memories, patterns, conversation_turns)\n#     \n#     def _is_ambiguous(self, query: str) -> bool:\n#         '''Detect ambiguous queries.'''\n#         # Very short queries\n#         if len(query.split()) <= 2:\n#             return True\n#         # Questions with no clear topic keywords\n#         keywords = graph.extract_keywords(query)\n#         if len(keywords) == 0:\n#             return True\n#         return False\n#\n# enhanced_selector = EnhancedModeSelector()\n#\n# test_queries = [\n#     \"Help?\",  # Should trigger CLARIFICATION\n#     \"What?\",  # Should trigger CLARIFICATION\n#     \"How do I deploy my Python app?\",  # Should NOT trigger CLARIFICATION\n# ]\n#\n# for q in test_queries:\n#     decision = enhanced_selector.select(q, sample_memories[:1], [])\n#     print(f\"'{q}' -> {decision.mode.value} ({decision.reason})\")\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nUncomment the exercises above one at a time and run them to practice!\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\"\"\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}