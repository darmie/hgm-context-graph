{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent AI State Management — Workshop Demo\n",
    "\n",
    "This notebook walks through three context management strategies, progressing from naive to sophisticated:\n",
    "\n",
    "1. **Phase 1: Sliding Window** — The problem: lost context\n",
    "2. **Phase 2: Summarization** — A partial fix: compressed history\n",
    "3. **Phase 3: Temperature-Scored Tiered Memory** — The full solution: self-organizing memory\n",
    "\n",
    "By the end, you'll see side-by-side how each strategy handles a 25-turn conversation and why tiered memory with temperature scoring produces the best recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Dependencies\n",
    "\n",
    "Install required packages. We use `sentence-transformers` for local embeddings (no API key needed for the embedding step) and `openai` for the LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy sentence-transformers tiktoken openai matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set your OpenAI API key (or use any compatible provider)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"  # Use a fast, cheap model for the demo\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"  # Local embedding model, no API key needed\n",
    "\n",
    "client = OpenAI()\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "print(f\"LLM: {LLM_MODEL}\")\n",
    "print(f\"Embeddings: {EMBED_MODEL} (local)\")\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Shared utilities used by all three phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(messages: list[dict]) -> int:\n",
    "    \"\"\"Count tokens in a list of chat messages.\"\"\"\n",
    "    total = 0\n",
    "    for m in messages:\n",
    "        total += len(tokenizer.encode(m.get(\"content\", \"\")))\n",
    "        total += 4  # role + formatting overhead per message\n",
    "    return total\n",
    "\n",
    "\n",
    "def llm_call(messages: list[dict]) -> str:\n",
    "    \"\"\"Make an LLM call and return the response text.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=300,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def embed(text: str) -> list[float]:\n",
    "    \"\"\"Generate embedding using local model.\"\"\"\n",
    "    return embedder.encode(text).tolist()\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))\n",
    "\n",
    "\n",
    "def extract_keywords(text: str) -> list[str]:\n",
    "    \"\"\"Simple keyword extraction — split and filter short/stop words.\"\"\"\n",
    "    stop = {\"i\", \"we\", \"the\", \"a\", \"an\", \"is\", \"are\", \"to\", \"do\", \"it\",\n",
    "            \"my\", \"our\", \"and\", \"or\", \"of\", \"on\", \"in\", \"at\", \"for\", \"with\",\n",
    "            \"have\", \"has\", \"that\", \"this\", \"what\", \"how\", \"why\", \"did\", \"say\"}\n",
    "    words = text.lower().replace(\"?\", \"\").replace(\".\", \"\").replace(\",\", \"\").split()\n",
    "    return [w for w in words if w not in stop and len(w) > 2]\n",
    "\n",
    "\n",
    "print(\"Helpers loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scenario\n",
    "\n",
    "A 25-turn DevOps conversation where a user progressively describes a production issue. Early turns establish key facts that are referenced by recall questions at the end.\n",
    "\n",
    "**Turns 1-20**: Build up context about the system, the problem, and the environment.  \n",
    "**Turns 21-25**: Ask recall questions that require information from early turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = [\n",
    "    # --- Context building (turns 0-19) ---\n",
    "    \"Hi, I'm Alex. I'm working on a Python microservice.\",                  # 0\n",
    "    \"We deploy to AWS using ECS.\",                                           # 1\n",
    "    \"The service handles payment processing with Stripe.\",                   # 2\n",
    "    \"We're seeing timeouts on the /checkout endpoint.\",                      # 3\n",
    "    \"The database is PostgreSQL on RDS, port 5432.\",                         # 4\n",
    "    \"Our CI/CD pipeline uses GitHub Actions.\",                               # 5\n",
    "    \"The team uses Docker with multi-stage builds.\",                         # 6\n",
    "    \"We recently added a Redis cache for sessions.\",                         # 7\n",
    "    \"The monitoring stack is Prometheus + Grafana.\",                         # 8\n",
    "    \"We suspect the Stripe webhook is causing the timeout.\",                 # 9\n",
    "    \"The webhook handler does a synchronous DB write.\",                      # 10\n",
    "    \"We tried increasing the timeout to 30s but it didn't help.\",           # 11\n",
    "    \"The error logs show connection pool exhaustion.\",                       # 12\n",
    "    \"Our pool size is set to 5 connections.\",                                # 13\n",
    "    \"Traffic spikes happen during lunch hours, 11am-1pm.\",                  # 14\n",
    "    \"We also have a batch job that runs at noon.\",                           # 15\n",
    "    \"The batch job processes refunds from the previous day.\",                # 16\n",
    "    \"It opens 3 long-running DB connections.\",                               # 17\n",
    "    \"So during peak, we have 3 batch + N request connections.\",              # 18\n",
    "    \"We're considering PgBouncer for connection pooling.\",                   # 19\n",
    "    # --- Recall questions (turns 20-24) ---\n",
    "    \"What cloud provider and service are we using?\",                         # 20 → AWS ECS\n",
    "    \"What's our current DB connection pool size?\",                           # 21 → 5\n",
    "    \"What payment processor do we use?\",                                     # 22 → Stripe\n",
    "    \"Why do we think the timeouts happen at lunch?\",                         # 23 → batch + traffic\n",
    "    \"Summarize the root cause and proposed solution.\",                       # 24 → pool exhaustion + PgBouncer\n",
    "]\n",
    "\n",
    "# Ground-truth answers for evaluation\n",
    "expected_answers = {\n",
    "    20: \"AWS ECS\",\n",
    "    21: \"5 connections\",\n",
    "    22: \"Stripe\",\n",
    "    23: \"batch job (3 connections) + lunch traffic spike causes pool exhaustion\",\n",
    "    24: \"connection pool exhaustion (5 pool + 3 batch at peak); proposed fix: PgBouncer\",\n",
    "}\n",
    "\n",
    "RECALL_START = 20\n",
    "print(f\"Scenario: {len(scenario)} turns ({RECALL_START} context + {len(scenario) - RECALL_START} recall questions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 1: Sliding Window\n",
    "\n",
    "The simplest approach — keep only the last N turns in the prompt. Everything older is discarded.\n",
    "\n",
    "**What to watch for**: When the recall questions arrive (turns 20-24), the sliding window will have lost all early facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowAgent:\n",
    "    \"\"\"Keeps the last `window_size` turns as context.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int = 5):\n",
    "        self.history: list[dict] = []\n",
    "        self.window_size = window_size\n",
    "        self.token_log: list[int] = []\n",
    "        self.responses: list[str] = []\n",
    "\n",
    "    def chat(self, user_message: str) -> tuple[str, int]:\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        # Only send the last N messages\n",
    "        window = self.history[-self.window_size * 2:]  # *2 to include assistant turns\n",
    "        tokens_used = count_tokens(window)\n",
    "        self.token_log.append(tokens_used)\n",
    "\n",
    "        response = llm_call(messages=window)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        self.responses.append(response)\n",
    "        return response, tokens_used\n",
    "\n",
    "\n",
    "print(\"SlidingWindowAgent defined (window_size=5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 1\n",
    "agent_sw = SlidingWindowAgent(window_size=5)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: SLIDING WINDOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, turn in enumerate(scenario):\n",
    "    response, tokens = agent_sw.chat(turn)\n",
    "    marker = \" <<< RECALL\" if i >= RECALL_START else \"\"\n",
    "    print(f\"\\nTurn {i:2d} [{tokens:4d} tokens]{marker}\")\n",
    "    print(f\"  User: {turn}\")\n",
    "    if i >= RECALL_START:\n",
    "        print(f\"  Agent: {response[:200]}\")\n",
    "        print(f\"  Expected: {expected_answers[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1 Observation\n",
    "\n",
    "Notice how the agent **cannot answer** the recall questions because the relevant facts (from turns 1-4, 13, etc.) fell outside the 5-turn window. The agent may guess or hallucinate.\n",
    "\n",
    "**Problem**: Fixed window = fixed amnesia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 2: Summarization\n",
    "\n",
    "Keep a rolling summary of older turns plus a small recent window. When history grows too long, summarize and compress.\n",
    "\n",
    "**What to watch for**: Better recall than sliding window, but summaries are lossy — specific details (exact numbers, port values) may be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationAgent:\n",
    "    \"\"\"Compresses older turns into a rolling summary.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int = 3):\n",
    "        self.history: list[dict] = []\n",
    "        self.summary: str = \"\"\n",
    "        self.window_size = window_size\n",
    "        self.token_log: list[int] = []\n",
    "        self.responses: list[str] = []\n",
    "\n",
    "    def _summarize(self, old_turns: list[dict]) -> str:\n",
    "        prompt = f\"Previous summary:\\n{self.summary}\\n\\nNew conversation turns:\\n\"\n",
    "        for t in old_turns:\n",
    "            prompt += f\"{t['role']}: {t['content']}\\n\"\n",
    "        prompt += (\"\\nWrite a concise summary preserving ALL key facts \"\n",
    "                   \"(names, numbers, services, ports, timestamps). \"\n",
    "                   \"Keep it under 200 words:\")\n",
    "        return llm_call(messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    def chat(self, user_message: str) -> tuple[str, int]:\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        # Summarize when history exceeds threshold\n",
    "        if len(self.history) > self.window_size * 2:\n",
    "            old_turns = self.history[:-self.window_size]\n",
    "            self.summary = self._summarize(old_turns)\n",
    "            self.history = self.history[-self.window_size:]\n",
    "\n",
    "        messages = []\n",
    "        if self.summary:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Summary of earlier conversation:\\n{self.summary}\"\n",
    "            })\n",
    "        messages.extend(self.history[-self.window_size * 2:])\n",
    "\n",
    "        tokens_used = count_tokens(messages)\n",
    "        self.token_log.append(tokens_used)\n",
    "\n",
    "        response = llm_call(messages=messages)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        self.responses.append(response)\n",
    "        return response, tokens_used\n",
    "\n",
    "\n",
    "print(\"SummarizationAgent defined (window_size=3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 2\n",
    "agent_sum = SummarizationAgent(window_size=3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: SUMMARIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, turn in enumerate(scenario):\n",
    "    response, tokens = agent_sum.chat(turn)\n",
    "    marker = \" <<< RECALL\" if i >= RECALL_START else \"\"\n",
    "    print(f\"\\nTurn {i:2d} [{tokens:4d} tokens]{marker}\")\n",
    "    print(f\"  User: {turn}\")\n",
    "    if i >= RECALL_START:\n",
    "        print(f\"  Agent: {response[:200]}\")\n",
    "        print(f\"  Expected: {expected_answers[i]}\")\n",
    "\n",
    "print(f\"\\nFinal summary ({len(agent_sum.summary)} chars):\")\n",
    "print(agent_sum.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 Observation\n",
    "\n",
    "Better recall — the summary retains key facts like \"AWS ECS\" and \"Stripe\". But notice:\n",
    "\n",
    "- **Lossy**: Specific numbers (pool size = 5, port 5432) may be compressed away\n",
    "- **No prioritization**: The summary treats all facts equally\n",
    "- **Extra LLM calls**: Every summarization step costs tokens and latency\n",
    "\n",
    "**Improvement needed**: What if we could automatically prioritize the facts that matter most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 3: Temperature-Scored Tiered Memory\n",
    "\n",
    "Every turn is stored as a memory with an embedding. On each query, we score all memories using a **temperature formula** that accounts for recency, relevance, frequency, and entity overlap. Memories are organized into tiers:\n",
    "\n",
    "| Tier | Temperature | Behavior |\n",
    "|------|------------|----------|\n",
    "| **HOT** | >= 0.70 | Injected into context, fast access |\n",
    "| **WARM** | 0.50 - 0.70 | Available for retrieval |\n",
    "| **COLD** | < 0.50 | Archived, rarely accessed |\n",
    "\n",
    "Memories automatically **promote** (cold -> warm -> hot) and **demote** (hot -> warm -> cold) based on usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Memory Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Memory:\n",
    "    \"\"\"A single memory unit with temperature and tier tracking.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    embedding: list[float]\n",
    "    memory_type: str              # SEMANTIC, EPISODIC, PROCEDURAL\n",
    "    temperature: float = 0.5\n",
    "    tier: str = \"cold\"            # hot, warm, cold\n",
    "    access_count: int = 0\n",
    "    last_accessed: datetime = field(default_factory=datetime.now)\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    entities: list[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "print(\"Memory dataclass defined.\")\n",
    "print(f\"Fields: {[f.name for f in Memory.__dataclass_fields__.values()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Temperature Scoring Formula\n",
    "\n",
    "The temperature of a memory is a weighted combination of five signals:\n",
    "\n",
    "```\n",
    "temperature = 0.30 * recency         (how recently was it accessed?)\n",
    "            + 0.25 * relevance        (cosine similarity to current query)\n",
    "            + 0.20 * frequency         (how often has it been accessed?)\n",
    "            + 0.15 * entity_overlap    (do query keywords match memory keywords?)\n",
    "            + 0.10 * agent_match       (was it created by the same agent?)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = {\n",
    "    \"recency\": 0.30,\n",
    "    \"relevance\": 0.25,\n",
    "    \"frequency\": 0.20,\n",
    "    \"entity_overlap\": 0.15,\n",
    "    \"agent_match\": 0.10,\n",
    "}\n",
    "\n",
    "\n",
    "def compute_temperature(\n",
    "    memory: Memory,\n",
    "    query_embedding: list[float],\n",
    "    query_entities: list[str],\n",
    "    max_access: int = 50,\n",
    ") -> float:\n",
    "    \"\"\"Compute the temperature score for a memory given a query.\"\"\"\n",
    "\n",
    "    # Recency: exponential decay based on hours since last access\n",
    "    hours_ago = (datetime.now() - memory.last_accessed).total_seconds() / 3600\n",
    "    recency = exp(-0.1 * hours_ago)\n",
    "\n",
    "    # Relevance: cosine similarity between query and memory embeddings\n",
    "    relevance = cosine_similarity(query_embedding, memory.embedding)\n",
    "\n",
    "    # Frequency: normalized access count\n",
    "    frequency = min(memory.access_count / max_access, 1.0)\n",
    "\n",
    "    # Entity overlap: fraction of query keywords found in memory\n",
    "    if query_entities and memory.entities:\n",
    "        overlap = len(set(query_entities) & set(memory.entities)) / len(set(query_entities))\n",
    "    else:\n",
    "        overlap = 0.0\n",
    "\n",
    "    # Agent match: always 1.0 in this single-agent demo\n",
    "    agent = 1.0\n",
    "\n",
    "    temp = (\n",
    "        WEIGHTS[\"recency\"] * recency\n",
    "        + WEIGHTS[\"relevance\"] * relevance\n",
    "        + WEIGHTS[\"frequency\"] * frequency\n",
    "        + WEIGHTS[\"entity_overlap\"] * overlap\n",
    "        + WEIGHTS[\"agent_match\"] * agent\n",
    "    )\n",
    "    return round(temp, 4)\n",
    "\n",
    "\n",
    "print(\"Temperature formula defined.\")\n",
    "print(f\"Weights: {WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tier Promotion & Demotion\n",
    "\n",
    "After scoring, memories move between tiers:\n",
    "- **temp >= 0.70** -> promote to HOT\n",
    "- **0.50 <= temp < 0.70** -> WARM\n",
    "- **temp < 0.50** -> demote to COLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIER_ORDER = [\"cold\", \"warm\", \"hot\"]\n",
    "\n",
    "\n",
    "def update_tiers(memories: list[Memory]) -> tuple[list, list]:\n",
    "    \"\"\"Update tier assignments and return lists of promotions and demotions.\"\"\"\n",
    "    promotions, demotions = [], []\n",
    "\n",
    "    for m in memories:\n",
    "        old_tier = m.tier\n",
    "        if m.temperature >= 0.70:\n",
    "            m.tier = \"hot\"\n",
    "        elif m.temperature >= 0.50:\n",
    "            m.tier = \"warm\"\n",
    "        else:\n",
    "            m.tier = \"cold\"\n",
    "\n",
    "        if m.tier != old_tier:\n",
    "            change = (m.id, m.text[:50], old_tier, m.tier, m.temperature)\n",
    "            if TIER_ORDER.index(m.tier) > TIER_ORDER.index(old_tier):\n",
    "                promotions.append(change)\n",
    "            else:\n",
    "                demotions.append(change)\n",
    "\n",
    "    return promotions, demotions\n",
    "\n",
    "\n",
    "print(\"Tier promotion/demotion logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Tiered Memory Agent\n",
    "\n",
    "This agent stores every turn as a memory, scores all memories on each query, promotes/demotes tiers, and injects only the top-k most relevant memories into the LLM prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TieredMemoryAgent:\n",
    "    \"\"\"Agent with temperature-scored tiered memory.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_fn, top_k: int = 10):\n",
    "        self.memories: list[Memory] = []\n",
    "        self.embed_fn = embed_fn\n",
    "        self.top_k = top_k\n",
    "        self.token_log: list[int] = []\n",
    "        self.responses: list[str] = []\n",
    "        self.tier_history: list[dict] = []\n",
    "\n",
    "    def store(self, text: str, entities: list[str], memory_type: str = \"EPISODIC\"):\n",
    "        \"\"\"Store a new memory in the cold tier.\"\"\"\n",
    "        emb = self.embed_fn(text)\n",
    "        mem = Memory(\n",
    "            id=f\"mem_{len(self.memories):03d}\",\n",
    "            text=text,\n",
    "            embedding=emb,\n",
    "            memory_type=memory_type,\n",
    "            entities=entities,\n",
    "        )\n",
    "        self.memories.append(mem)\n",
    "\n",
    "    def recall(self, query: str, query_entities: list[str]):\n",
    "        \"\"\"Score memories, update tiers, return top-k.\"\"\"\n",
    "        query_emb = self.embed_fn(query)\n",
    "\n",
    "        # Score all memories\n",
    "        for m in self.memories:\n",
    "            m.temperature = compute_temperature(m, query_emb, query_entities)\n",
    "\n",
    "        # Update tiers\n",
    "        promotions, demotions = update_tiers(self.memories)\n",
    "        self.tier_history.append({\n",
    "            \"promotions\": promotions,\n",
    "            \"demotions\": demotions,\n",
    "            \"tier_counts\": self._tier_counts(),\n",
    "        })\n",
    "\n",
    "        # Bump access stats for retrieved memories\n",
    "        for m in self.memories:\n",
    "            if m.tier in (\"hot\", \"warm\"):\n",
    "                m.access_count += 1\n",
    "                m.last_accessed = datetime.now()\n",
    "\n",
    "        ranked = sorted(self.memories, key=lambda m: m.temperature, reverse=True)\n",
    "        return ranked[:self.top_k], promotions, demotions\n",
    "\n",
    "    def chat(self, user_message: str) -> tuple[str, int, list, list]:\n",
    "        \"\"\"Store turn, recall relevant memories, generate response.\"\"\"\n",
    "        entities = extract_keywords(user_message)\n",
    "        self.store(user_message, entities)\n",
    "\n",
    "        results, promotions, demotions = self.recall(user_message, entities)\n",
    "\n",
    "        # Build context from top memories with tier labels\n",
    "        context_lines = []\n",
    "        for m in results:\n",
    "            context_lines.append(f\"[{m.tier.upper()} t={m.temperature:.2f}] {m.text}\")\n",
    "        context = \"\\n\".join(context_lines)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"Relevant memories (sorted by importance):\\n{context}\"},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "\n",
    "        tokens_used = count_tokens(messages)\n",
    "        self.token_log.append(tokens_used)\n",
    "\n",
    "        response = llm_call(messages=messages)\n",
    "        self.responses.append(response)\n",
    "        return response, tokens_used, promotions, demotions\n",
    "\n",
    "    def _tier_counts(self) -> dict:\n",
    "        counts = {\"hot\": 0, \"warm\": 0, \"cold\": 0}\n",
    "        for m in self.memories:\n",
    "            counts[m.tier] += 1\n",
    "        return counts\n",
    "\n",
    "\n",
    "print(\"TieredMemoryAgent defined (top_k=10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 3\n",
    "agent_tm = TieredMemoryAgent(embed_fn=embed, top_k=10)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 3: TIERED MEMORY WITH TEMPERATURE SCORING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, turn in enumerate(scenario):\n",
    "    response, tokens, promos, demos = agent_tm.chat(turn)\n",
    "    marker = \" <<< RECALL\" if i >= RECALL_START else \"\"\n",
    "    print(f\"\\nTurn {i:2d} [{tokens:4d} tokens]{marker}\")\n",
    "    print(f\"  User: {turn}\")\n",
    "\n",
    "    if promos:\n",
    "        for p in promos:\n",
    "            print(f\"  PROMOTED: {p[1]} ({p[2]} -> {p[3]}, temp={p[4]:.2f})\")\n",
    "    if demos:\n",
    "        for d in demos:\n",
    "            print(f\"  DEMOTED:  {d[1]} ({d[2]} -> {d[3]}, temp={d[4]:.2f})\")\n",
    "\n",
    "    if i >= RECALL_START:\n",
    "        print(f\"  Agent: {response[:200]}\")\n",
    "        print(f\"  Expected: {expected_answers[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3 Observation\n",
    "\n",
    "The tiered memory agent:\n",
    "- **Promotes** relevant memories when they match the query (e.g., AWS/ECS facts get promoted when asked about cloud provider)\n",
    "- **Injects only the top-k** most relevant memories, keeping token usage bounded\n",
    "- **Retains specific details** (pool size = 5, port 5432) because they're stored as individual memories, not compressed into a lossy summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comparison Dashboard\n",
    "\n",
    "Let's compare all three approaches side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Usage Over Turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(agent_sw.token_log, label=\"Sliding Window\", marker=\"o\", markersize=4)\n",
    "ax.plot(agent_sum.token_log, label=\"Summarization\", marker=\"s\", markersize=4)\n",
    "ax.plot(agent_tm.token_log, label=\"Tiered Memory\", marker=\"^\", markersize=4)\n",
    "\n",
    "ax.axvline(x=RECALL_START, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Recall questions start\")\n",
    "ax.set_xlabel(\"Turn\")\n",
    "ax.set_ylabel(\"Tokens Sent to LLM\")\n",
    "ax.set_title(\"Token Usage by Context Strategy\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total tokens — Sliding: {sum(agent_sw.token_log):,} | \"\n",
    "      f\"Summary: {sum(agent_sum.token_log):,} | \"\n",
    "      f\"Tiered: {sum(agent_tm.token_log):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier Distribution Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns = range(len(agent_tm.tier_history))\n",
    "hot_counts = [h[\"tier_counts\"][\"hot\"] for h in agent_tm.tier_history]\n",
    "warm_counts = [h[\"tier_counts\"][\"warm\"] for h in agent_tm.tier_history]\n",
    "cold_counts = [h[\"tier_counts\"][\"cold\"] for h in agent_tm.tier_history]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.stackplot(\n",
    "    turns, hot_counts, warm_counts, cold_counts,\n",
    "    labels=[\"Hot\", \"Warm\", \"Cold\"],\n",
    "    colors=[\"#ef4444\", \"#f59e0b\", \"#3b82f6\"],\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.axvline(x=RECALL_START, color=\"black\", linestyle=\"--\", alpha=0.5, label=\"Recall questions\")\n",
    "ax.set_xlabel(\"Turn\")\n",
    "ax.set_ylabel(\"Number of Memories\")\n",
    "ax.set_title(\"Memory Tier Distribution Over Time\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RECALL ACCURACY COMPARISON (Turns 20-24)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Turn':<6} {'Question':<45} {'Expected':<25}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for turn_idx in range(RECALL_START, len(scenario)):\n",
    "    resp_idx = turn_idx  # response index matches turn index\n",
    "    print(f\"\\nTurn {turn_idx}: {scenario[turn_idx]}\")\n",
    "    print(f\"  Expected: {expected_answers[turn_idx]}\")\n",
    "    print(f\"  Sliding:  {agent_sw.responses[resp_idx][:120]}\")\n",
    "    print(f\"  Summary:  {agent_sum.responses[resp_idx][:120]}\")\n",
    "    print(f\"  Tiered:   {agent_tm.responses[resp_idx][:120]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Memory State (Tiered Agent)\n",
    "\n",
    "Let's inspect which memories ended up in each tier after the full conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL MEMORY STATE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for tier in [\"hot\", \"warm\", \"cold\"]:\n",
    "    tier_mems = [m for m in agent_tm.memories if m.tier == tier]\n",
    "    print(f\"\\n{'=' * 5} {tier.upper()} ({len(tier_mems)} memories) {'=' * 5}\")\n",
    "    for m in sorted(tier_mems, key=lambda x: x.temperature, reverse=True):\n",
    "        print(f\"  [{m.id}] temp={m.temperature:.3f} access={m.access_count:2d} | {m.text[:70]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Key Takeaways\n",
    "\n",
    "| Strategy | Token Efficiency | Recall Quality | Self-Organizing | Complexity |\n",
    "|----------|-----------------|---------------|-----------------|------------|\n",
    "| **Sliding Window** | Bounded but wasteful | Poor (loses early context) | No | Low |\n",
    "| **Summarization** | Good (compressed) | Moderate (lossy) | No | Medium |\n",
    "| **Tiered Memory** | Best (only relevant) | Best (individual recall) | Yes | Higher |\n",
    "\n",
    "1. **State is what makes an agent an agent** — without memory management, you have a chatbot with amnesia\n",
    "2. **Temperature scoring turns memory into a continuous optimization problem** — not binary keep/discard\n",
    "3. **Tier promotion/demotion creates self-organizing memory** — frequently accessed facts stay hot\n",
    "4. **Match your strategy to your use case** — sliding window for simple chat, tiered memory for autonomous agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
